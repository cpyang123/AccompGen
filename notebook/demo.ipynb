{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5cf1e7-c275-4929-9c44-ec48e26a2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import torch\n",
    "import random\n",
    "import bisect\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import GPT2Model, GPT2LMHeadModel, GPT2Config, LlamaModel, LlamaForCausalLM, PreTrainedModel \n",
    "from samplings import top_p_sampling, top_k_sampling, temperature_sampling\n",
    "from abctoolkit.utils import Exclaim_re, Quote_re, SquareBracket_re, Barline_regexPattern\n",
    "from abctoolkit.transpose import Note_list, Pitch_sign_list\n",
    "from abctoolkit.duration import calculate_bartext_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00fd2ebb-6e53-4038-af85-f9c5f02fde0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for inference\n",
    "INFERENCE_WEIGHTS_PATH = '../weights/weights_notagenx_p_size_16_p_length_1024_p_layers_20_h_size_1280.pth'               # Path to weights for inference# Folder to save output files\n",
    "TOP_K = 9                                                       # Top k for sampling\n",
    "TOP_P = 0.9                                                      # Top p for sampling\n",
    "TEMPERATURE = 1.2                                                 # Temperature for sampling\n",
    "\n",
    "# Configurations for model\n",
    "PATCH_STREAM = True                                             # Stream training / inference\n",
    "PATCH_SIZE = 16                                                # Patch Size\n",
    "PATCH_LENGTH = 1024                                             # Patch Length\n",
    "CHAR_NUM_LAYERS = 6                                             # Number of layers in the decoder\n",
    "PATCH_NUM_LAYERS = 20                                           # Number of layers in the encoder\n",
    "HIDDEN_SIZE = 1280                                               # Hidden Size\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb70eb19-8b9c-4864-b711-7a0395b42c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchilizer:\n",
    "    def __init__(self, stream=PATCH_STREAM):\n",
    "        self.stream = stream\n",
    "        self.delimiters = [\"|:\", \"::\", \":|\", \"[|\", \"||\", \"|]\", \"|\"]\n",
    "        self.regexPattern = '(' + '|'.join(map(re.escape, self.delimiters)) + ')'\n",
    "        self.bos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        self.special_token_id = 0\n",
    "\n",
    "    def split_bars(self, body_lines):\n",
    "        \"\"\"\n",
    "        Split a body of music into individual bars.\n",
    "        \"\"\"\n",
    "        new_bars = []\n",
    "        try:\n",
    "            for line in body_lines:\n",
    "                line_bars = re.split(self.regexPattern, line)\n",
    "                line_bars = list(filter(None, line_bars))\n",
    "                new_line_bars = []\n",
    "\n",
    "                if len(line_bars) == 1:\n",
    "                    new_line_bars = line_bars\n",
    "                else:\n",
    "                    if line_bars[0] in self.delimiters:\n",
    "                        new_line_bars = [line_bars[i] + line_bars[i + 1] for i in range(0, len(line_bars), 2)]\n",
    "                    else:\n",
    "                        new_line_bars = [line_bars[0]] + [line_bars[i] + line_bars[i + 1] for i in range(1, len(line_bars), 2)]\n",
    "                    if 'V' not in new_line_bars[-1]:\n",
    "                        new_line_bars[-2] += new_line_bars[-1]  # 吸收最后一个 小节线+\\n 的组合\n",
    "                        new_line_bars = new_line_bars[:-1]\n",
    "                new_bars += new_line_bars\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return new_bars\n",
    "\n",
    "    def split_patches(self, abc_text, patch_size=PATCH_SIZE, generate_last=False):\n",
    "        if not generate_last and len(abc_text) % patch_size != 0:\n",
    "            abc_text += chr(self.eos_token_id)\n",
    "        patches = [abc_text[i : i + patch_size] for i in range(0, len(abc_text), patch_size)]\n",
    "        return patches\n",
    "\n",
    "    def patch2chars(self, patch):\n",
    "        \"\"\"\n",
    "        Convert a patch into a bar.\n",
    "        \"\"\"\n",
    "        bytes = ''\n",
    "        for idx in patch:\n",
    "            if idx == self.eos_token_id:\n",
    "                break\n",
    "            if idx < self.eos_token_id:\n",
    "                pass\n",
    "            bytes += chr(idx)\n",
    "        return bytes\n",
    "        \n",
    "\n",
    "    def patchilize_metadata(self, metadata_lines):\n",
    "\n",
    "        metadata_patches = []\n",
    "        for line in metadata_lines:\n",
    "            metadata_patches += self.split_patches(line)\n",
    "\n",
    "        return metadata_patches\n",
    "    \n",
    "    def patchilize_tunebody(self, tunebody_lines, encode_mode='train'):\n",
    "\n",
    "        tunebody_patches = []\n",
    "        bars = self.split_bars(tunebody_lines)\n",
    "        if encode_mode == 'train':\n",
    "            for bar in bars:\n",
    "                tunebody_patches += self.split_patches(bar)\n",
    "        elif encode_mode == 'generate':\n",
    "            for bar in bars[:-1]:\n",
    "                tunebody_patches += self.split_patches(bar)\n",
    "            tunebody_patches += self.split_patches(bars[-1], generate_last=True)\n",
    "       \n",
    "        return tunebody_patches\n",
    "\n",
    "    def encode_train(self, abc_text, patch_length=PATCH_LENGTH, patch_size=PATCH_SIZE, add_special_patches=True, cut=True):\n",
    "\n",
    "        lines = abc_text.split('\\n')\n",
    "        lines = list(filter(None, lines))\n",
    "        lines = [line + '\\n' for line in lines]\n",
    "\n",
    "        tunebody_index = -1\n",
    "        for i, line in enumerate(lines):\n",
    "            if '[V:' in line:\n",
    "                tunebody_index = i\n",
    "                break\n",
    "\n",
    "        metadata_lines = lines[ : tunebody_index]\n",
    "        tunebody_lines = lines[tunebody_index : ]\n",
    "\n",
    "        if self.stream:\n",
    "            tunebody_lines = ['[r:' + str(line_index) + '/' + str(len(tunebody_lines) - line_index - 1) + ']' + line for line_index, line in\n",
    "                                enumerate(tunebody_lines)]    \n",
    "\n",
    "        metadata_patches = self.patchilize_metadata(metadata_lines)\n",
    "        tunebody_patches = self.patchilize_tunebody(tunebody_lines, encode_mode='train')\n",
    "\n",
    "        if add_special_patches:\n",
    "            bos_patch = chr(self.bos_token_id) * (patch_size - 1) + chr(self.eos_token_id)\n",
    "            eos_patch = chr(self.bos_token_id) + chr(self.eos_token_id) * (patch_size - 1)\n",
    "\n",
    "            metadata_patches = [bos_patch] + metadata_patches\n",
    "            tunebody_patches = tunebody_patches + [eos_patch]\n",
    "\n",
    "        if self.stream:\n",
    "            if len(metadata_patches) + len(tunebody_patches) > patch_length:\n",
    "                available_cut_indexes = [0] + [index + 1 for index, patch in enumerate(tunebody_patches) if '\\n' in patch]\n",
    "                line_index_for_cut_index = list(range(len(available_cut_indexes)))  \n",
    "                end_index = len(metadata_patches) + len(tunebody_patches) - patch_length\n",
    "                biggest_index = bisect.bisect_left(available_cut_indexes, end_index) \n",
    "                available_cut_indexes = available_cut_indexes[:biggest_index + 1]\n",
    "\n",
    "                if len(available_cut_indexes) == 1:\n",
    "                    choices = ['head']\n",
    "                elif len(available_cut_indexes) == 2:\n",
    "                    choices = ['head', 'tail']\n",
    "                else:\n",
    "                    choices = ['head', 'tail', 'middle']\n",
    "                choice = random.choice(choices)\n",
    "                if choice == 'head':\n",
    "                    patches = metadata_patches + tunebody_patches[0:]\n",
    "                else:\n",
    "                    if choice == 'tail':\n",
    "                        cut_index = len(available_cut_indexes) - 1\n",
    "                    else:\n",
    "                        cut_index = random.choice(range(1, len(available_cut_indexes) - 1))\n",
    "\n",
    "                    line_index = line_index_for_cut_index[cut_index] \n",
    "                    stream_tunebody_lines = tunebody_lines[line_index : ]\n",
    "                    \n",
    "                    stream_tunebody_patches = self.patchilize_tunebody(stream_tunebody_lines, encode_mode='train')\n",
    "                    if add_special_patches:\n",
    "                        stream_tunebody_patches = stream_tunebody_patches + [eos_patch]\n",
    "                    patches = metadata_patches + stream_tunebody_patches\n",
    "            else:\n",
    "                patches = metadata_patches + tunebody_patches\n",
    "        else:\n",
    "            patches = metadata_patches + tunebody_patches\n",
    "\n",
    "        if cut: \n",
    "            patches = patches[ : patch_length]\n",
    "        else:   \n",
    "            pass\n",
    "\n",
    "        # encode to ids\n",
    "        id_patches = []\n",
    "        for patch in patches:\n",
    "            id_patch = [ord(c) for c in patch] + [self.special_token_id] * (patch_size - len(patch))\n",
    "            id_patches.append(id_patch)\n",
    "\n",
    "        return id_patches\n",
    "\n",
    "    def encode_generate(self, abc_code, patch_length=PATCH_LENGTH, patch_size=PATCH_SIZE, add_special_patches=True):\n",
    "\n",
    "        lines = abc_code.split('\\n')\n",
    "        lines = list(filter(None, lines))\n",
    "    \n",
    "        tunebody_index = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.startswith('[V:') or line.startswith('[r:'):\n",
    "                tunebody_index = i\n",
    "                break\n",
    "    \n",
    "        metadata_lines = lines[ : tunebody_index]\n",
    "        tunebody_lines = lines[tunebody_index : ]   \n",
    "    \n",
    "        metadata_lines = [line + '\\n' for line in metadata_lines]\n",
    "        if self.stream:\n",
    "            if not abc_code.endswith('\\n'):\n",
    "                tunebody_lines = [tunebody_lines[i] + '\\n' for i in range(len(tunebody_lines) - 1)] + [tunebody_lines[-1]]\n",
    "            else:\n",
    "                tunebody_lines = [tunebody_lines[i] + '\\n' for i in range(len(tunebody_lines))]\n",
    "        else:\n",
    "            tunebody_lines = [line + '\\n' for line in tunebody_lines]\n",
    "    \n",
    "        metadata_patches = self.patchilize_metadata(metadata_lines)\n",
    "        tunebody_patches = self.patchilize_tunebody(tunebody_lines, encode_mode='generate')\n",
    "    \n",
    "        if add_special_patches:\n",
    "            bos_patch = chr(self.bos_token_id) * (patch_size - 1) + chr(self.eos_token_id)\n",
    "\n",
    "            metadata_patches = [bos_patch] + metadata_patches\n",
    "    \n",
    "        patches = metadata_patches + tunebody_patches\n",
    "        patches = patches[ : patch_length]\n",
    "\n",
    "        # encode to ids\n",
    "        id_patches = []\n",
    "        for patch in patches:\n",
    "            if len(patch) < PATCH_SIZE and patch[-1] != chr(self.eos_token_id):\n",
    "                id_patch = [ord(c) for c in patch]\n",
    "            else:\n",
    "                id_patch = [ord(c) for c in patch] + [self.special_token_id] * (patch_size - len(patch))\n",
    "            id_patches.append(id_patch)\n",
    "        \n",
    "        return id_patches\n",
    "\n",
    "    def decode(self, patches):\n",
    "        \"\"\"\n",
    "        Decode patches into music.\n",
    "        \"\"\"\n",
    "        return ''.join(self.patch2chars(patch) for patch in patches)\n",
    "\n",
    "\n",
    "class PatchLevelDecoder(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    A Patch-level Decoder model for generating patch features in an auto-regressive manner. \n",
    "    It inherits PreTrainedModel from transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.patch_embedding = torch.nn.Linear(PATCH_SIZE * 128, config.n_embd)\n",
    "        torch.nn.init.normal_(self.patch_embedding.weight, std=0.02)\n",
    "        self.base = GPT2Model(config)\n",
    "\n",
    "    def forward(self,\n",
    "                patches: torch.Tensor,\n",
    "                masks=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The forward pass of the patch-level decoder model.\n",
    "        :param patches: the patches to be encoded\n",
    "        :param masks: the masks for the patches\n",
    "        :return: the encoded patches\n",
    "        \"\"\"\n",
    "        patches = torch.nn.functional.one_hot(patches, num_classes=128).to(self.dtype)\n",
    "        patches = patches.reshape(len(patches), -1, PATCH_SIZE * (128))\n",
    "        patches = self.patch_embedding(patches.to(self.device))\n",
    "\n",
    "        if masks==None:\n",
    "            return self.base(inputs_embeds=patches)\n",
    "        else:\n",
    "            return self.base(inputs_embeds=patches,\n",
    "                             attention_mask=masks)\n",
    "\n",
    "\n",
    "class CharLevelDecoder(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    A Char-level Decoder model for generating the chars within each patch in an auto-regressive manner\n",
    "    based on the encoded patch features. It inherits PreTrainedModel from transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.special_token_id = 0\n",
    "        self.bos_token_id = 1\n",
    "\n",
    "        self.base = GPT2LMHeadModel(config)\n",
    "\n",
    "    def forward(self,\n",
    "                encoded_patches: torch.Tensor,\n",
    "                target_patches: torch.Tensor):\n",
    "        \"\"\"\n",
    "        The forward pass of the char-level decoder model.\n",
    "        :param encoded_patches: the encoded patches\n",
    "        :param target_patches: the target patches\n",
    "        :return: the output of the model\n",
    "        \"\"\"\n",
    "        # preparing the labels for model training\n",
    "        target_patches = torch.cat((torch.ones_like(target_patches[:,0:1])*self.bos_token_id, target_patches), dim=1)\n",
    "        # print('target_patches shape:', target_patches.shape)\n",
    "\n",
    "        target_masks = target_patches == self.special_token_id\n",
    "        labels = target_patches.clone().masked_fill_(target_masks, -100)\n",
    "\n",
    "        # masking the labels for model training\n",
    "        target_masks = torch.ones_like(labels)\n",
    "        target_masks = target_masks.masked_fill_(labels == -100, 0)\n",
    "\n",
    "        # select patches\n",
    "        if PATCH_SAMPLING_BATCH_SIZE!=0 and PATCH_SAMPLING_BATCH_SIZE<target_patches.shape[0]:\n",
    "            indices = list(range(len(target_patches)))\n",
    "            random.shuffle(indices)\n",
    "            selected_indices = sorted(indices[:PATCH_SAMPLING_BATCH_SIZE])\n",
    "\n",
    "            target_patches = target_patches[selected_indices,:]\n",
    "            target_masks = target_masks[selected_indices,:]\n",
    "            encoded_patches = encoded_patches[selected_indices,:]\n",
    "\n",
    "        # get input embeddings\n",
    "        inputs_embeds = torch.nn.functional.embedding(target_patches, self.base.transformer.wte.weight)\n",
    "\n",
    "        # concatenate the encoded patches with the input embeddings\n",
    "        inputs_embeds = torch.cat((encoded_patches.unsqueeze(1), inputs_embeds[:,1:,:]), dim=1)\n",
    "\n",
    "        output = self.base(inputs_embeds=inputs_embeds, \n",
    "                         attention_mask=target_masks,\n",
    "                         labels=labels)\n",
    "                         # output_hidden_states=True=True)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def generate(self,\n",
    "                 encoded_patch: torch.Tensor,   # [hidden_size]\n",
    "                 tokens: torch.Tensor): # [1]\n",
    "        \"\"\"\n",
    "        The generate function for generating a patch based on the encoded patch and already generated tokens.\n",
    "        :param encoded_patch: the encoded patch\n",
    "        :param tokens: already generated tokens in the patch\n",
    "        :return: the probability distribution of next token\n",
    "        \"\"\"\n",
    "        encoded_patch = encoded_patch.reshape(1, 1, -1) # [1, 1, hidden_size]\n",
    "        tokens = tokens.reshape(1, -1)\n",
    "\n",
    "        # Get input embeddings\n",
    "        tokens = torch.nn.functional.embedding(tokens, self.base.transformer.wte.weight)\n",
    "\n",
    "        # Concatenate the encoded patch with the input embeddings\n",
    "        tokens = torch.cat((encoded_patch, tokens[:,1:,:]), dim=1)\n",
    "        \n",
    "        # Get output from model\n",
    "        outputs = self.base(inputs_embeds=tokens)\n",
    "        \n",
    "        # Get probabilities of next token\n",
    "        probs = torch.nn.functional.softmax(outputs.logits.squeeze(0)[-1], dim=-1)\n",
    "\n",
    "        return probs\n",
    "\n",
    "class NotaGenLMHeadModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    NotaGen is a language model with a hierarchical structure.\n",
    "    It includes a patch-level decoder and a char-level decoder.\n",
    "    The patch-level decoder is used to generate patch features in an auto-regressive manner.\n",
    "    The char-level decoder is used to generate the chars within each patch in an auto-regressive manner.\n",
    "    It inherits PreTrainedModel from transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_config, decoder_config):\n",
    "        super().__init__(encoder_config)\n",
    "        self.special_token_id = 0\n",
    "        self.bos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        self.patch_level_decoder = PatchLevelDecoder(encoder_config)\n",
    "        self.char_level_decoder = CharLevelDecoder(decoder_config)\n",
    "\n",
    "    def forward(self,\n",
    "                patches: torch.Tensor,\n",
    "                masks: torch.Tensor):\n",
    "        \"\"\"\n",
    "        The forward pass of the bGPT model.\n",
    "        :param patches: the patches to be encoded\n",
    "        :param masks: the masks for the patches\n",
    "        :return: the decoded patches\n",
    "        \"\"\"\n",
    "        patches = patches.reshape(len(patches), -1, PATCH_SIZE)\n",
    "        encoded_patches = self.patch_level_decoder(patches, masks)[\"last_hidden_state\"]\n",
    "        \n",
    "        left_shift_masks = masks * (masks.flip(1).cumsum(1).flip(1) > 1)\n",
    "        masks[:, 0] = 0\n",
    "        \n",
    "        encoded_patches = encoded_patches[left_shift_masks == 1]\n",
    "        patches = patches[masks == 1]        \n",
    "\n",
    "        return self.char_level_decoder(encoded_patches, patches)\n",
    "        \n",
    "    def generate(self,\n",
    "                 patches: torch.Tensor,\n",
    "                 top_k=0,\n",
    "                 top_p=1,\n",
    "                 temperature=1.0):\n",
    "        \"\"\"\n",
    "        The generate function for generating patches based on patches.\n",
    "        :param patches: the patches to be encoded\n",
    "        :param top_k: the top k for sampling\n",
    "        :param top_p: the top p for sampling\n",
    "        :param temperature: the temperature for sampling\n",
    "        :return: the generated patches\n",
    "        \"\"\"\n",
    "        if patches.shape[-1] % PATCH_SIZE != 0:\n",
    "            tokens = patches[:,:,-(patches.shape[-1]%PATCH_SIZE):].squeeze(0, 1)\n",
    "            tokens = torch.cat((torch.tensor([self.bos_token_id], device=self.device), tokens), dim=-1)\n",
    "            patches = patches[:,:,:-(patches.shape[-1]%PATCH_SIZE)]\n",
    "        else:\n",
    "            tokens =  torch.tensor([self.bos_token_id], device=self.device)\n",
    "\n",
    "        patches = patches.reshape(len(patches), -1, PATCH_SIZE) # [bs, seq, patch_size]\n",
    "        encoded_patches = self.patch_level_decoder(patches)[\"last_hidden_state\"]    # [bs, seq, hidden_size]\n",
    "        generated_patch = []            \n",
    "\n",
    "        while True:\n",
    "            prob = self.char_level_decoder.generate(encoded_patches[0][-1], tokens).cpu().detach().numpy()  # [128]\n",
    "            prob = top_k_sampling(prob, top_k=top_k, return_probs=True) # [128]\n",
    "            prob = top_p_sampling(prob, top_p=top_p, return_probs=True) # [128]\n",
    "            token = temperature_sampling(prob, temperature=temperature) # int\n",
    "            char = chr(token)\n",
    "            generated_patch.append(token)\n",
    "\n",
    "            if len(tokens) >= PATCH_SIZE:# or token == self.eos_token_id:\n",
    "                break\n",
    "            else:\n",
    "                tokens = torch.cat((tokens, torch.tensor([token], device=self.device)), dim=0)\n",
    "        \n",
    "        return generated_patch\n",
    "\n",
    "def clean_to_abc(raw_text, unreduce=True, output_path='output.abc'):\n",
    "    # Remove [r:x/y] tags\n",
    "    cleaned = re.sub(r'\\[r:\\d+/\\d+\\]', '', raw_text)\n",
    "\n",
    "    # Add required ABC headers\n",
    "    lines = cleaned.strip().splitlines()\n",
    "    header_inserted = False\n",
    "    abc_lines = []\n",
    "    for line in lines:\n",
    "        if not header_inserted and line.startswith('%%score'):\n",
    "            abc_lines.insert(0, 'T:Generated\\n')\n",
    "            abc_lines.insert(0, 'X:1\\n')\n",
    "            header_inserted = True\n",
    "        abc_lines.append(line if line.endswith('\\n') else line + '\\n')\n",
    "\n",
    "    # Optional: fill missing rests\n",
    "    if unreduce:\n",
    "        try:\n",
    "            abc_lines = rest_unreduce(abc_lines)\n",
    "        except Exception as e:\n",
    "            print(\"Unreduce failed:\", e)\n",
    "\n",
    "    # Save to .abc file\n",
    "    Path(output_path).write_text(''.join(abc_lines), encoding='utf-8')\n",
    "    print(f\"Saved cleaned ABC to {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d126533-a9a1-48a5-9b1b-be6da37a55ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Number: 515738880\n"
     ]
    }
   ],
   "source": [
    "Note_list = Note_list + ['z', 'x']\n",
    "\n",
    "patchilizer = Patchilizer()\n",
    "\n",
    "patch_config = GPT2Config(num_hidden_layers=PATCH_NUM_LAYERS,\n",
    "                          max_length=PATCH_LENGTH,\n",
    "                          max_position_embeddings=PATCH_LENGTH,\n",
    "                          n_embd=HIDDEN_SIZE,\n",
    "                          num_attention_heads=HIDDEN_SIZE // 64,\n",
    "                          vocab_size=1)\n",
    "byte_config = GPT2Config(num_hidden_layers=CHAR_NUM_LAYERS,\n",
    "                         max_length=PATCH_SIZE + 1,\n",
    "                         max_position_embeddings=PATCH_SIZE + 1,\n",
    "                         hidden_size=HIDDEN_SIZE,\n",
    "                         num_attention_heads=HIDDEN_SIZE // 64,\n",
    "                         vocab_size=128)\n",
    "\n",
    "model = NotaGenLMHeadModel(encoder_config=patch_config, decoder_config=byte_config).to(device)\n",
    "\n",
    "def prepare_model_for_kbit_training(model, use_gradient_checkpointing=True):\n",
    "    \"\"\"\n",
    "    Prepare model for k-bit training.\n",
    "    Features include:\n",
    "    1. Convert model to mixed precision (FP16).\n",
    "    2. Disable unnecessary gradient computations.\n",
    "    3. Enable gradient checkpointing (optional).\n",
    "    \"\"\"\n",
    "    # Convert model to mixed precision\n",
    "    model = model.to(dtype=torch.float16)\n",
    "\n",
    "    # Disable gradients for embedding layers\n",
    "    for param in model.parameters():\n",
    "        if param.dtype == torch.float32:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Enable gradient checkpointing\n",
    "    if use_gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=False  \n",
    ")\n",
    "\n",
    "print(\"Parameter Number: \" + str(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "checkpoint = torch.load(INFERENCE_WEIGHTS_PATH, map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def complete_brackets(s):\n",
    "    stack = []\n",
    "    bracket_map = {'{': '}', '[': ']', '(': ')'}\n",
    "    \n",
    "    # Iterate through each character, handle bracket matching\n",
    "    for char in s:\n",
    "        if char in bracket_map:\n",
    "            stack.append(char)\n",
    "        elif char in bracket_map.values():\n",
    "            # Find the corresponding left bracket\n",
    "            for key, value in bracket_map.items():\n",
    "                if value == char:\n",
    "                    if stack and stack[-1] == key:\n",
    "                        stack.pop()\n",
    "                    break  # Found matching right bracket, process next character\n",
    "    \n",
    "    # Complete missing right brackets (in reverse order of remaining left brackets in stack)\n",
    "    completion = ''.join(bracket_map[c] for c in reversed(stack))\n",
    "    return s + completion\n",
    "\n",
    "\n",
    "def rest_unreduce(abc_lines):\n",
    "\n",
    "    tunebody_index = None\n",
    "    for i in range(len(abc_lines)):\n",
    "        if abc_lines[i].startswith('%%score'):\n",
    "            abc_lines[i] = complete_brackets(abc_lines[i])\n",
    "        if '[V:' in abc_lines[i]:\n",
    "            tunebody_index = i\n",
    "            break\n",
    "\n",
    "    metadata_lines = abc_lines[: tunebody_index]\n",
    "    tunebody_lines = abc_lines[tunebody_index:]\n",
    "\n",
    "    part_symbol_list = []\n",
    "    voice_group_list = []\n",
    "    for line in metadata_lines:\n",
    "        if line.startswith('%%score'):\n",
    "            for round_bracket_match in re.findall(r'\\((.*?)\\)', line):\n",
    "                voice_group_list.append(round_bracket_match.split())\n",
    "            existed_voices = [item for sublist in voice_group_list for item in sublist]\n",
    "        if line.startswith('V:'):\n",
    "            symbol = line.split()[0]\n",
    "            part_symbol_list.append(symbol)\n",
    "            if symbol[2:] not in existed_voices:\n",
    "                voice_group_list.append([symbol[2:]])\n",
    "    z_symbol_list = []  # voices that use z as rest\n",
    "    x_symbol_list = []  # voices that use x as rest\n",
    "    for voice_group in voice_group_list:\n",
    "        z_symbol_list.append('V:' + voice_group[0])\n",
    "        for j in range(1, len(voice_group)):\n",
    "            x_symbol_list.append('V:' + voice_group[j])\n",
    "\n",
    "    part_symbol_list.sort(key=lambda x: int(x[2:]))\n",
    "\n",
    "    unreduced_tunebody_lines = []\n",
    "\n",
    "    for i, line in enumerate(tunebody_lines):\n",
    "        unreduced_line = ''\n",
    "\n",
    "        line = re.sub(r'^\\[r:[^\\]]*\\]', '', line)\n",
    "\n",
    "        pattern = r'\\[V:(\\d+)\\](.*?)(?=\\[V:|$)'\n",
    "        matches = re.findall(pattern, line)\n",
    "\n",
    "        line_bar_dict = {}\n",
    "        for match in matches:\n",
    "            key = f'V:{match[0]}'\n",
    "            value = match[1]\n",
    "            line_bar_dict[key] = value\n",
    "\n",
    "        # calculate duration and collect barline\n",
    "        dur_dict = {}  \n",
    "        for symbol, bartext in line_bar_dict.items():\n",
    "            right_barline = ''.join(re.split(Barline_regexPattern, bartext)[-2:])\n",
    "            bartext = bartext[:-len(right_barline)]\n",
    "            try:\n",
    "                bar_dur = calculate_bartext_duration(bartext)\n",
    "            except:\n",
    "                bar_dur = None\n",
    "            if bar_dur is not None:\n",
    "                if bar_dur not in dur_dict.keys():\n",
    "                    dur_dict[bar_dur] = 1\n",
    "                else:\n",
    "                    dur_dict[bar_dur] += 1\n",
    "\n",
    "        try:\n",
    "            ref_dur = max(dur_dict, key=dur_dict.get)\n",
    "        except:\n",
    "            pass    # use last ref_dur\n",
    "\n",
    "        if i == 0:\n",
    "            prefix_left_barline = line.split('[V:')[0]\n",
    "        else:\n",
    "            prefix_left_barline = ''\n",
    "\n",
    "        for symbol in part_symbol_list:\n",
    "            if symbol in line_bar_dict.keys():\n",
    "                symbol_bartext = line_bar_dict[symbol]\n",
    "            else:\n",
    "                if symbol in z_symbol_list:\n",
    "                    symbol_bartext = prefix_left_barline + 'z' + str(ref_dur) + right_barline\n",
    "                elif symbol in x_symbol_list:\n",
    "                    symbol_bartext = prefix_left_barline + 'x' + str(ref_dur) + right_barline\n",
    "            unreduced_line += '[' + symbol + ']' + symbol_bartext\n",
    "\n",
    "        unreduced_tunebody_lines.append(unreduced_line + '\\n')\n",
    "\n",
    "    unreduced_lines = metadata_lines + unreduced_tunebody_lines\n",
    "\n",
    "    return unreduced_lines\n",
    "\n",
    "\n",
    "def inference_patch(period, composer, instrumentation, additional_prompts=[], tune_body = []):\n",
    "\n",
    "    prompt_lines=[\n",
    "    '%' + period + '\\n',\n",
    "    '%' + composer + '\\n',\n",
    "    '%' + instrumentation + '\\n']\n",
    "\n",
    "    for prompt in additional_prompts:\n",
    "        prompt_lines.append(prompt)\n",
    "\n",
    "    for prompt in tune_body:\n",
    "        prompt_lines.append(prompt)\n",
    "\n",
    "    tunebody_flag = tune_body != []\n",
    "\n",
    "    while True:\n",
    "\n",
    "        failure_flag = False\n",
    "\n",
    "        bos_patch = [patchilizer.bos_token_id] * (PATCH_SIZE - 1) + [patchilizer.eos_token_id]\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        prompt_patches = patchilizer.patchilize_metadata(prompt_lines)\n",
    "        byte_list = list(''.join(prompt_lines))\n",
    "        context_tunebody_byte_list = []\n",
    "        metadata_byte_list = []\n",
    "\n",
    "        print(''.join(byte_list), end='')\n",
    "\n",
    "        prompt_patches = [[ord(c) for c in patch] + [patchilizer.special_token_id] * (PATCH_SIZE - len(patch)) for patch\n",
    "                          in prompt_patches]\n",
    "        prompt_patches.insert(0, bos_patch)\n",
    "\n",
    "        input_patches = torch.tensor(prompt_patches, device=device).reshape(1, -1)\n",
    "\n",
    "        end_flag = False\n",
    "        cut_index = None\n",
    "\n",
    "        tunebody_flag = tunebody_flag == True\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            while True:\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    predicted_patch = model.generate(input_patches.unsqueeze(0),\n",
    "                                                    top_k=TOP_K,\n",
    "                                                    top_p=TOP_P,\n",
    "                                                    temperature=TEMPERATURE)\n",
    "                if not tunebody_flag and patchilizer.decode([predicted_patch]).startswith('[r:'):  # 初次进入tunebody，必须以[r:0/开头\n",
    "                    tunebody_flag = True\n",
    "                    r0_patch = torch.tensor([ord(c) for c in '[r:0/']).unsqueeze(0).to(device)\n",
    "                    temp_input_patches = torch.concat([input_patches, r0_patch], axis=-1)\n",
    "                    predicted_patch = model.generate(temp_input_patches.unsqueeze(0),\n",
    "                                                    top_k=TOP_K,\n",
    "                                                    top_p=TOP_P,\n",
    "                                                    temperature=TEMPERATURE)\n",
    "                    predicted_patch = [ord(c) for c in '[r:0/'] + predicted_patch\n",
    "                if predicted_patch[0] == patchilizer.bos_token_id and predicted_patch[1] == patchilizer.eos_token_id:\n",
    "                    end_flag = True\n",
    "                    break\n",
    "                next_patch = patchilizer.decode([predicted_patch])\n",
    "\n",
    "                for char in next_patch:\n",
    "                    byte_list.append(char)\n",
    "                    if tunebody_flag:\n",
    "                        context_tunebody_byte_list.append(char)\n",
    "                    else:\n",
    "                        metadata_byte_list.append(char)\n",
    "                    print(char, end='')\n",
    "\n",
    "                patch_end_flag = False\n",
    "                for j in range(len(predicted_patch)):\n",
    "                    if patch_end_flag:\n",
    "                        predicted_patch[j] = patchilizer.special_token_id\n",
    "                    if predicted_patch[j] == patchilizer.eos_token_id:\n",
    "                        patch_end_flag = True\n",
    "\n",
    "                predicted_patch = torch.tensor([predicted_patch], device=device)  # (1, 16)\n",
    "                input_patches = torch.cat([input_patches, predicted_patch], dim=1)  # (1, 16 * patch_len)\n",
    "\n",
    "                if len(byte_list) > 102400:\n",
    "                    failure_flag = True\n",
    "                    break\n",
    "                if time.time() - start_time > 10 * 60: \n",
    "                    failure_flag = True\n",
    "                    break\n",
    "\n",
    "                if input_patches.shape[1] >= PATCH_LENGTH * PATCH_SIZE and not end_flag:\n",
    "                    print('Stream generating...')\n",
    "\n",
    "                    metadata = ''.join(metadata_byte_list)\n",
    "                    context_tunebody = ''.join(context_tunebody_byte_list)\n",
    "\n",
    "                    if '\\n' not in context_tunebody:\n",
    "                        break   # Generated content is all metadata, abandon\n",
    "\n",
    "                    context_tunebody_liness = context_tunebody.split('\\n')\n",
    "                    if not context_tunebody.endswith('\\n'):\n",
    "                        context_tunebody_liness = [context_tunebody_liness[i] + '\\n' for i in range(len(context_tunebody_liness) - 1)] + [context_tunebody_liness[-1]]\n",
    "                    else:\n",
    "                        context_tunebody_liness = [context_tunebody_liness[i] + '\\n' for i in range(len(context_tunebody_liness))]\n",
    "\n",
    "                    cut_index = len(context_tunebody_liness) // 2\n",
    "                    abc_code_slice = metadata + ''.join(context_tunebody_liness[-cut_index:])\n",
    "\n",
    "                    input_patches = patchilizer.encode_generate(abc_code_slice)\n",
    "\n",
    "                    input_patches = [item for sublist in input_patches for item in sublist]\n",
    "                    input_patches = torch.tensor([input_patches], device=device)\n",
    "                    input_patches = input_patches.reshape(1, -1)\n",
    "\n",
    "                    context_tunebody_byte_list = list(''.join(context_tunebody_lines[-cut_index:]))\n",
    "\n",
    "            if not failure_flag:\n",
    "                abc_text = ''.join(byte_list)\n",
    "\n",
    "                # unreduce\n",
    "                abc_lines = abc_text.split('\\n')\n",
    "                abc_lines = list(filter(None, abc_lines))\n",
    "                abc_lines = [line + '\\n' for line in abc_lines]\n",
    "                try:\n",
    "                    unreduced_abc_lines = rest_unreduce(abc_lines)\n",
    "                except:\n",
    "                    failure_flag = True\n",
    "                    pass\n",
    "                else:\n",
    "                    unreduced_abc_lines = [line for line in unreduced_abc_lines if not(line.startswith('%') and not line.startswith('%%'))]\n",
    "                    unreduced_abc_lines = ['X:1\\n'] + unreduced_abc_lines\n",
    "                    unreduced_abc_text = ''.join(unreduced_abc_lines)\n",
    "                    return unreduced_abc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d128e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.3.0\n",
      "cuda available: True\n",
      "cuda version built: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda version built:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36826e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period: Baroque, Composer: Vivaldi, Antonio, Instrumentation: Orchestral\n",
      "%Baroque\n",
      "%Vivaldi, Antonio\n",
      "%Orchestral\n",
      "\n",
      "%Orchestral\n",
      "%end\n",
      "%%score 1 [ 2 3 4 5 ]\n",
      "L:1/8\n",
      "Q:1/4=60\n",
      "M:4/4\n",
      "K:Bb\n",
      "V:1 treble nm=\"Violino \\nprincipale\"\n",
      "V:2 treble nm=\"Violino I\\nripieno\"\n",
      "V:3 treble nm=\"Violino II\\nRipieno\"\n",
      "V:4 alto nm=\"Viola\"\n",
      "V:5 bass nm=\"Basso\"\n",
      "[r:0/93][V:1]\"^Allegro\" d|\n",
      "[r:1/92][V:1]gGBd g/b/a/g/ d'/b/a/g/|[V:2]z4 z2 z d|[V:3]z4 z2 z B|[V:4]z4 z2 z G|[V:5]z4 z2 z G,|\n",
      "[r:2/91][V:1]^f2 z d a/g/f/=e/ d/c'/b/a/|[V:2]aD^FA d2 z d|[V:3]d2 z ^f a2 z f|[V:4]^F2 z d F2 z A|[V:5]DD,^F,A, D2 z D,|\n",
      "[r:3/90][V:1]bg z d g/f/e/d/ c/b/a/g/|[V:2]gGBd g2 z g|[V:3]d2 z B d2 z =e|[V:4]B2 z B B2 z c|[V:5]G,G,,B,,D, G,2 z C,|\n",
      "[r:4/89][V:1]af z c f/e/d/c/ B/f/e/d/|[V:2]c'FAc f2 z f|[V:3]f2 z f a2 z d|[V:4]A2 z A A2 z B|[V:5]F,F,,A,,C,"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 88\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeriod: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperiod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Composer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomposer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Instrumentation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstrumentation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minference_patch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomposer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstrumentation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     abc_lines \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m     91\u001b[0m     abc_lines \u001b[38;5;241m=\u001b[39m [line \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m abc_lines \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()]  \u001b[38;5;66;03m# Add newlines and remove empty lines\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 206\u001b[0m, in \u001b[0;36minference_patch\u001b[0;34m(period, composer, instrumentation, additional_prompts)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[0;32m--> 206\u001b[0m         predicted_patch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_patches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOP_K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOP_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tunebody_flag \u001b[38;5;129;01mand\u001b[39;00m patchilizer\u001b[38;5;241m.\u001b[39mdecode([predicted_patch])\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[r:\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# 初次进入tunebody，必须以[r:0/开头\u001b[39;00m\n\u001b[1;32m    211\u001b[0m         tunebody_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 381\u001b[0m, in \u001b[0;36mNotaGenLMHeadModel.generate\u001b[0;34m(self, patches, top_k, top_p, temperature)\u001b[0m\n\u001b[1;32m    378\u001b[0m generated_patch \u001b[38;5;241m=\u001b[39m []            \n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchar_level_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_patches\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# [128]\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     prob \u001b[38;5;241m=\u001b[39m top_k_sampling(prob, top_k\u001b[38;5;241m=\u001b[39mtop_k, return_probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# [128]\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     prob \u001b[38;5;241m=\u001b[39m top_p_sampling(prob, top_p\u001b[38;5;241m=\u001b[39mtop_p, return_probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# [128]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 313\u001b[0m, in \u001b[0;36mCharLevelDecoder.generate\u001b[0;34m(self, encoded_patch, tokens)\u001b[0m\n\u001b[1;32m    310\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((encoded_patch, tokens[:,\u001b[38;5;241m1\u001b[39m:,:]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Get output from model\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Get probabilities of next token\u001b[39;00m\n\u001b[1;32m    316\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1305\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1305\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1119\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         output_attentions,\n\u001b[1;32m   1117\u001b[0m     )\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:617\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    615\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    616\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 617\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    626\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:347\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    345\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    350\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:217\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    214\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "periods = {\n",
    "    \"Baroque\": [\n",
    "        # \"Bach, Johann Sebastian\",\n",
    "        # \"Corelli, Arcangelo\",\n",
    "        # \"Handel, George Frideric\",\n",
    "        # \"Scarlatti, Domenico\",\n",
    "        \"Vivaldi, Antonio\"\n",
    "    ],\n",
    "    \"Classical\": [\n",
    "        \"Beethoven, Ludwig van\",\n",
    "        \"Haydn, Joseph\",\n",
    "        \"Mozart, Wolfgang Amadeus\",\n",
    "        \"Paradis, Maria Theresia von\",\n",
    "        \"Reichardt, Louise\",\n",
    "        \"Saint-Georges, Joseph Bologne\",\n",
    "        \"Schroter, Corona\"\n",
    "    ],\n",
    "    \"Romantic\": [\n",
    "        \"Bartok, Bela\",\n",
    "        \"Berlioz, Hector\",\n",
    "        \"Bizet, Georges\",\n",
    "        \"Boulanger, Lili\",\n",
    "        \"Boulton, Harold\",\n",
    "        \"Brahms, Johannes\",\n",
    "        \"Burgmuller, Friedrich\",\n",
    "        \"Butterworth, George\",\n",
    "        \"Chaminade, Cecile\",\n",
    "        \"Chausson, Ernest\",\n",
    "        \"Chopin, Frederic\",\n",
    "        \"Cornelius, Peter\",\n",
    "        \"Debussy, Claude\",\n",
    "        \"Dvorak, Antonin\",\n",
    "        \"Faisst, Clara\",\n",
    "        \"Faure, Gabriel\",\n",
    "        \"Franz, Robert\",\n",
    "        \"Gonzaga, Chiquinha\",\n",
    "        \"Grandval, Clemence de\",\n",
    "        \"Grieg, Edvard\",\n",
    "        \"Hensel, Fanny\",\n",
    "        \"Holmes, Augusta Mary Anne\",\n",
    "        \"Jaell, Marie\",\n",
    "        \"Kinkel, Johanna\",\n",
    "        \"Kralik, Mathilde\",\n",
    "        \"Lang, Josephine\",\n",
    "        \"Lehmann, Liza\",\n",
    "        \"Liszt, Franz\",\n",
    "        \"Mayer, Emilie\",\n",
    "        \"Medtner, Nikolay\",\n",
    "        \"Mendelssohn, Felix\",\n",
    "        \"Munktell, Helena\",\n",
    "        \"Parratt, Walter\",\n",
    "        \"Prokofiev, Sergey\",\n",
    "        \"Rachmaninoff, Sergei\",\n",
    "        \"Ravel, Maurice\",\n",
    "        \"Saint-Saens, Camille\",\n",
    "        \"Satie, Erik\",\n",
    "        \"Schubert, Franz\",\n",
    "        \"Schumann, Clara\",\n",
    "        \"Schumann, Robert\",\n",
    "        \"Scriabin, Aleksandr\",\n",
    "        \"Shostakovich, Dmitry\",\n",
    "        \"Sibelius, Jean\",\n",
    "        \"Smetana, Bedrich\",\n",
    "        \"Tchaikovsky, Pyotr\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def sanitize(name: str) -> str:\n",
    "    \"\"\"Convert to safe filename (remove spaces, commas, etc.).\"\"\"\n",
    "    return re.sub(r'[^A-Za-z0-9]+', '_', name).strip('_')\n",
    "\n",
    "instrumentations = [\n",
    "                    # \"Keyboard\",\n",
    "                    \"Orchestral\", \n",
    "                    \"Choral\", \n",
    "                    \"Chamber\", \n",
    "                    \"Art Song\"\n",
    "                    ]\n",
    "\n",
    "for period, composers in periods.items():\n",
    "    for composer in composers:\n",
    "        for instrumentation in instrumentations:\n",
    "            print(f\"Period: {period}, Composer: {composer}, Instrumentation: {instrumentation}\")\n",
    "            \n",
    "\n",
    "            try:\n",
    "                result = inference_patch(period, composer, instrumentation)\n",
    "\n",
    "                abc_lines = result.splitlines()\n",
    "                abc_lines = [line + '\\n' for line in abc_lines if line.strip()]  # Add newlines and remove empty lines\n",
    "\n",
    "                abc_lines = rest_unreduce(abc_lines)\n",
    "\n",
    "                # Construct unique file name\n",
    "                filename = f\"{sanitize(period)}_{sanitize(composer)}_{sanitize(instrumentation)}.abc\"\n",
    "                \n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.writelines(abc_lines)\n",
    "                !python abc2xml.py -o . {filename}\n",
    "            except Exception as e:\n",
    "                print(f\"Some error {e} has occured, skipping{filename}\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "502c4420-533b-43cc-80ca-b2c94cd4be04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%Classical\n",
      "%Beethoven, Ludwig van\n",
      "%Keyboard\n",
      "%%score { ( 1 2 ) | ( 3 4 ) } \n",
      "L:1/8\n",
      "Q:1/4=60\n",
      "M:3/4\n",
      "K:C\n",
      "V:1 treble nm=\"piano\"\n",
      "V:2 treble\n",
      "V:3 bass nm=\"piano\"\n",
      "V:4 bass\n",
      "[r:0/80][V:1]\"^Andante con moto\"!p! g4{a} (gf/e/)|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\n",
      "[r:1/79][V:1]^cd) d2 z2|[V:3](B,/G/D/G/) (B,/G/D/G/) (B,/G/D/G/)|\n",
      "[r:2/78][V:1](d3 e/f/ g/a/g/f/)|[V:3](B,/G/D/G/) (B,/G/D/G/) (B,/G/D/G/)|\n",
      "[r:3/77][V:1](^de) e2 z2|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\n",
      "[r:4/76][V:1]!f! (c'>a) (c'7/2 a/4f/4)|[V:3][K:treble] (C/A/F/A/) (C/A/F/A/) (C/A/F/A/)|[V:4][K:treble] x6|\n",
      "[r:5/75][V:1](c'>g) (c'7/2 g/4e/4)|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\n",
      "[r:6/74][V:1]!p! (g/f/e/d/) c2{e} (d/c/d/e/)|[V:3](A,/F/D/F/) (G,/E/C/E/) (G,/F/D/F/)|\n",
      "[r:7/73][V:1]!f! c2 z/ .c/.d/.e/ .f/.g/.a/.b/|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\n",
      "[r:8/72][V:1]!p! (c'>a) (c'7/2 a/4f/4)|[V:3](C/A/F/A/) (C/A/F/A/) (C/A/F/A/)|\n",
      "[r:9/71][V:1](c'>g) (c'7/2 g/4e/4)|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\n",
      "[r:10/70][V:1](g/f/e/d/) c2{e} (d/c/d/e/)|[V:3](A,/F/D/F/) (G,/E/C/E/) (G,/F/D/F/)|\n",
      "[r:11/69][V:1]c2 z!f! .[G,E].[A,F].[B,G]|[V:3](C/E/G/E/) C z z2|\n",
      "[r:12/68][V:1].[CE].[EG] ([EG]>[FA] [EG]/[CE]/[FA]/[DF]/)|[V:3][K:bass] C,2 z2 z2|[V:4][K:bass] x6|\n",
      "[r:13/67][V:1]([DF][CE]) z!p! .[CE].[DF].[EG]|[V:3]C,2 C,,2 z2|\n",
      "[r:14/66][V:1]([EG]/[DF]/)([FA]/[EG]/) ([EG]/[DF]/)([DF]/[CE]/) ([CE]/[B,D]/[DF]/[CE]/)|[V:3]z2 z2 z ^F,|\n",
      "[r:15/65][V:1]([CE][B,D]) z!f! .[G,E].[A,F].[B,G]|[V:3]G,2 G,,2 z2|\n",
      "[r:16/64][V:1].[CE].[EG] ([EG]>[FA] [EG]/[CE]/[FA]/[DF]/)|[V:3]C,2 z2 z2|\n",
      "[r:17/63][V:1]([DF][CE]) z!p! .[CE].[DF].[EG]|[V:3]C,2 C,,2 z2|\n",
      "[r:18/62][V:1]([EG]/[DF]/)([FA]/[EG]/) ([EG]/[DF]/)([DF]/[CE]/) ([CE]/[B,D]/[A,C]/[G,B,]/)|[V:3]z2 z2 G,,2|\n",
      "[r:19/61][V:1][B,D]C z/!f! (C/B,/C/ D/C/B,/C/)|[V:3]C,2 C,,2 z2|\n",
      "[r:20/60][V:1](^C/D/E/D/ F/E/G/F/ A/G/B/A/)|[V:3]z2 z2 z (G,/F,/)|\n",
      "[r:21/59][V:1](c2 B/)!p!(c/B/c/ d/c/B/c/)|[V:3](E,/F,/G,/F,/ E,) z z2|\n",
      "[r:22/58][V:1](^c/d/e/d/ f/e/g/f/ a/g/b/a/)|[V:3][K:treble] z2 z2 z (G/F/)|[V:4][K:treble] x6|\n",
      "[r:23/57][V:1](c'2 b/)(c'/b/c'/ d'/c'/b/c'/)|[V:3](E/F/G/F/ E) z z2|\n",
      "[r:24/56][V:1]!f! d'3 (c'/b/) (a/g/^f/e/)|[V:3](B,/G/D/G/) (B,/G/D/G/) (B,/G/D/G/)|\n",
      "[r:25/55][V:1](d/^c/)(e/d/) d2 (d/=c/B/c/)|[V:3](C/^F/D/F/) (C/F/D/F/) (A,/F/D/F/)|\n",
      "[r:26/54][V:1]B(d'/b/) (c'/a/)(b/g/) (a/^f/)(g/e/)|[V:3][G,G]2 z2 z2|\n",
      "[r:27/53][V:1](^d/e/^f/e/) e2 (=d/c/B/c/)|[V:3](C/^F/D/F/) (C/F/D/F/) (A,/F/D/F/)|\n",
      "[r:28/52][V:1]B(d'/b/) (c'/a/)(b/g/) (a/^f/)(g/e/)|[V:3][G,G]2 z2 z2|\n",
      "[r:29/51][V:1](^d/e/^f/e/) e2 (=d/c/B/c/)|[V:3](C/^F/D/F/) (C/F/D/F/) (A,/F/D/F/)|\n",
      "[r:30/50][V:1](B/c/^c/d/) (d/=c/B/A/) GTA{GA}|[V:3]G,/G/A,/G/ B,/G/C/G/ D/^F/C/F/|\n",
      "[r:31/49][V:1](B/c/^c/d/) (d/=c/B/A/) GTA{GA}|[V:3]G,/G/A,/G/ B,/G/C/G/ D/^F/C/F/|\n",
      "[r:32/48][V:1](B/d/g/b/) (b/a/g/^f/ e/d/c/B/)|[V:3]G,2 z2 z2|\n",
      "[r:33/47][V:1](B/A/c/e/) G2 (BA)|[V:3][K:bass] z2 B,2 C2|[V:4][K:bass] C,4 D,2|\n",
      "[r:34/46][V:1](^A2 B2) z2|[V:3](C2 B,2) z2|[V:4]G,4 x2|\n",
      "[r:35/45][V:1](b/c'/^c'/d'/) (d'/=c'/b/a/ g/^f/e/d/)|[V:3]G,2 z2 z2|\n",
      "[r:36/44][V:1](d/c/e/a/) g2 (T^f3/2e/4f/4)|[V:3][K:treble] z [CA] [DB][DB][Dc][Dc]|[V:4][K:treble] x6|\n",
      "[r:37/43][V:1](g/d/c'/a/) g2 (T^f3/2e/4f/4)|[V:3][GB][CA] [DB][DB][Dc][Dc]|\n",
      "[r:38/42][V:1](g/d/d'/c'/) (b/d'/)(^f/a/) (g/b/)(^d/f/)|[V:3][GB]2 z2 z2|\n",
      "[r:39/41][V:1](e/g/c/e/) (d/g/B/d/) (c/e/A/c/)|[V:3][K:bass] z6|[V:4][K:bass] x6|\n",
      "[r:40/40][V:1](B/g/^f/e/ d/c/B/A/ G)T^F{EF}|[V:3][D,B,]2 [D,B,]2 [D,C]2|\n",
      "[r:41/39][V:1](G/D/d/c/) (B/d/^F/A/) (G/B/^D/F/)|[V:3][G,B,]2 z2 z2|\n",
      "[r:42/38][V:1](E/G/C/E/) (D/G/B,/D/) (C/E/A,/C/)|[V:3]z2 z2 [C,,C,]2|\n",
      "[r:43/37][V:1](B,/G/^F/E/ D/C/B,/A,/ G,T^F,){E,F,}|[V:3][D,,D,]2 [D,,D,]2 [D,,C,]2|\n",
      "[r:44/36][V:1](^F,4 G,) z::[V:3]([G,,-C,]4 [G,,B,,]) z::\n",
      "[r:45/35][V:1]z2 z/!p! .G/.A/.B/ .c/.d/.e/.^f/|\n",
      "[r:46/34][V:1]g4 (a/g/f/e/)|[V:3](A,/^C/E/C/) (A,/C/E/C/) (A,/C/E/C/)|\n",
      "[r:47/33][V:1](^cd) d2 z2|[V:3](A,/D/F/D/) (A,/D/F/D/) (A,/D/F/D/)|\n",
      "[r:48/32][V:1](f4 g/f/e/d/)|[V:3](G,/B,/D/B,/) (G,/B,/D/B,/) (G,/B,/D/B,/)|\n",
      "[r:49/31][V:1](Bc) c2 z2|[V:3](G,/C/E/C/) (G,/C/E/C/) (G,/C/E/C/)|\n",
      "[r:50/30][V:1](_e2 dc_BA)|[V:3](^F,/C/A,/C/) (F,/D/A,/D/) (F,/D/A,/D/)|\n",
      "[r:51/29][V:1](d2 c_BAG)|[V:3](G,/D/_B,/D/) (G,/D/B,/D/) (_E,/C/G,/C/)|\n",
      "[r:52/28][V:1](_B/A/c/A/) G2{B} (A/G/A/B/)|[V:3](D,/_B,/G,/B,/) (D,/B,/G,/B,/) (D,/C/^F,/C/)|\n",
      "[r:53/27][V:1](G3/2^F/4G/4) (_A3/2G/4A/4) (G/=F/_E/D/)|[V:3]([G,_B,]2 [F,D]2) z2|\n",
      "[r:54/26][V:1](^CD_EFG_A)|[V:3]z (_A,G,F,_E,D,)|\n",
      "[r:55/25][V:1](A/_B/c/B/) B2 z2|[V:3](_E,/D,/E,/F,/) (G,/F,/G,/F,/ E,/D,/C,/B,,/)|\n",
      "[r:56/24][V:1](_e/d/c/_B/ _A/G/F/_E/ D/C/_B,/A,/)|[V:3](C,6|\n",
      "[r:57/23][V:1](C2 B,)([_EG][DF][CE])|[V:3]G,,2) z2 z2|\n",
      "[r:58/22][V:1]([C_E]2 [B,D])([EG][DF][CE])|[V:3]G,,2 z2 z2|\n",
      "[r:59/21][V:1][C_E][B,D] z!f! .[G,=E].[A,F].[B,G]|[V:3]G,,2 G,2 z2|\n",
      "[r:60/20][V:1].[CE].[EG] ([EG]>[FA] [EG]/[CE]/[FA]/[DF]/)|[V:3]C,2 z2 z2|\n",
      "[r:61/19][V:1]([DF][CE]) z!p! .[CE].[DF].[EG]|[V:3]C,2 C,,2 z2|\n",
      "[r:62/18][V:1]([EG]/[DF]/)([FA]/[EG]/) ([EG]/[DF]/)([DF]/[CE]/) ([CE]/[B,D]/[DF]/[CE]/)|[V:3]z2 z2 z ^F,|\n",
      "[r:63/17][V:1]([CE][B,D]) z!f! .[G,E].[A,F].[B,G]|[V:3]G,2 G,,2 z2|\n",
      "[r:64/16][V:1].[CE].[EG] ([EG]>[FA] [EG]/[CE]/[FA]/[DF]/)|[V:3]C,2 z2 z2|\n",
      "[r:65/15][V:1]([DF][CE]) z!p! .[CE].[DF].[EG]|[V:3]C,2 C,,2 z2|\n",
      "[r:66/14][V:1]([EG]/[DF]/)([FA]/[EG]/) ([EG]/[DF]/)([DF]/[CE]/) ([CE]/[B,D]/[A,C]/[G,B,]/)|[V:3]z2 z2 G,,2|\n",
      "[r:67/13][V:1]([B,D]C) z/!f! (C/B,/C/ D/C/B,/C/)|[V:3]C,2 C,,2 z2|\n",
      "[r:68/12][V:1](^C/D/E/D/ F/E/G/F/ A/G/_B/A/)|[V:3]z2 z2 z (G,/F,/)|\n",
      "[r:69/11][V:1](d2 ^c/)!p!(d/c/d/ e/d/c/d/)|[V:3](E,/F,/G,/F,/ E,) z z2|\n",
      "[r:70/10][V:1](e/f/g/f/ a/g/_b/a/ ^c'/=b/d'/c'/)|[V:3][K:treble] z2 z2 z (_B/A/)|[V:4][K:treble] x6|\n",
      "[r:71/9][V:1](f'2 e'/)(f'/e'/f'/ g'/f'/e'/f'/)|[V:3](G/A/_B/A/ G) z z2|\n",
      "[r:72/8][V:1]!f! g'3 (f'/e'/) (d'/c'/b/a/)|[V:3](E/c/G/c/) (E/c/G/c/) (E/c/G/c/)|\n",
      "[r:73/7][V:1](g/^f/)(a/g/) g2 (g/=f/e/f/)|[V:3](F/B/G/B/) (F/B/G/B/) (D/B/G/B/)|\n",
      "[r:74/6][V:1]e(g'/e'/) (f'/d'/)(e'/c'/) (d'/b/)(c'/a/)|[V:3][Cc]2 z2 z2|\n",
      "[r:75/5][V:1](^g/a/b/a/) a2 (=g/f/e/f/)|[V:3](F/B/G/B/) (F/B/G/B/) (D/B/G/B/)|\n",
      "[r:76/4][V:1]e(g'/e'/) (f'/d'/)(e'/c'/) (d'/b/)(c'/a/)|[V:3][Cc]2 z2 z2|\n",
      "[r:77/3][V:1](^g/a/b/a/) a2 (=g/f/e/f/)|[V:3](F/B/G/B/) (F/B/G/B/) (D/B/G/B/)|\n",
      "[r:78/2][V:1](e/f/^f/g/) (g/=f/e/d/) cTB{cd}|[V:3]C/c/D/c/ E/c/F/c/ G/B/F/B/|\n",
      "[r:79/1][V:1](e/f/^f/g/) (g/=f/e/d/) cTB{AB}|[V:3]C/c/D/c/ E/c/F/c/ G/B/F/B/|\n",
      "[r:80/0][V:1](c/G/A/B/ c/d/e/f/) (^f/g/a/g/)|][V:3]C2 z2 z2|]\n",
      "-- decoded from utf-8\n",
      "-- skipped header: (field X,1)\n",
      "./fine_tuned_Classical_Beethoven_Ludwig_van_Keyboard_0.xml written\n",
      "-- done in 0.46 secs\n",
      "%Classical\n",
      "%Beethoven, Ludwig van\n",
      "%Keyboard\n",
      "%%score { ( 1 2 ) | ( 3 4 ) } \n",
      "L:1/8\n",
      "Q:1/4=60\n",
      "M:3/4\n",
      "K:C\n",
      "V:1 treble nm=\"piano\"\n",
      "V:2 treble\n",
      "V:3 bass nm=\"piano\"\n",
      "V:4 bass\n",
      "[r:0/80][V:1]\"^Andante con moto\"!p! g4{a} (gf/e/)|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\n",
      "[r:1/79][V:1].f2) z2 z2|[V:3](B,/G/D/G/) (B,/G/D/G/) (B,/G/D/G/)|\n",
      "[r:2/78][V:1]f3 (g/f/ e/d/c/B/)|[V:3](B,/G/D/G/) (B,/G/D/G/) (B,/G/D/G/)|\n",
      "[r:3/77][V:1](d2 .e2) z2|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\n",
      "[r:4/76][V:1](e/d/e/f/ ^c/d/e/d/ g/f/e/d/)|[V:3][F,A,]6|\n",
      "[r:5/75][V:1](d/c/d/e/ B/c/d/c/ f/e/d/c/)|[V:3][E,G,]6|\n",
      "[r:6/74][V:1](B/c/^G/A/ =G/F/E/D/ ^C/D/F/D/)|[V:3][F,A,]6|\n",
      "[r:7/73][V:1](C4 B,) z|[V:3]G,6|\n",
      "[r:8/72][V:1]z (G ^FG ce)|[V:3][K:treble] (C/E/) z z2 z2|[V:4][K:treble] x6|\n",
      "[r:9/71][V:1](g>e) .c2 z2|[V:3](C/E/G/E/) (C/E/G/E/) (C/E/G/E/)|\n",
      "[r:10/70][V:1](^c/d/f/d/) (=c2 B/c/e/c/)|[V:3](B,/D/G/D/) (C/E/G/E/) (C/E/G/E/)|\n",
      "[r:11/69][V:1](e2 .d2) z2|[V:3](G,/C/E/C/) (G,/B,/D/B,/) (G,/B,/D/B,/)|\n",
      "[r:12/68][V:1]z/ (G/A/B/ c/d/e/^f/ g/a/b/c'/)|[V:3]B,6|\n",
      "[r:13/67][V:1](^g/a/c'/e'/ =g>)(g b/a/c'/a/)|[V:3]([Bd]2 [ce]2) [^Fd]2|\n",
      "[r:14/66][V:1](a2 g)(d ^c=c)|[V:3](c2 B) (B ^A=A)|[V:4]G3 x3|\n",
      "[r:15/65][V:1]B z z2 z2|[V:3](G/^F/G/A/ B/c/d/c/ B/A/G/F/)|\n",
      "[r:16/64][V:1]z/ (g/a/b/ c'/d'/e'/^f'/ g'/f'/g'/e'/)|[V:3]E6|\n",
      "[r:17/63][V:1](d'2 ^c'/)(g'/e'/c'/ d'/)(b/=c'/a/)|[V:3]z/ (E/G/^A/) [D=A]2 z [^Fd]|\n",
      "[r:18/62][V:1].g([ca]/4[Bg]/4[A^f]/4[Bg]/4) .[db]([ec']/4[db]/4[^c^a]/4[db]/4) .[gd']([^f=c']/4[e=c']/4[db]/4[ec']/4)|[V:3][GB] z [G,B,] z [G,C] z|\n",
      "[r:19/61][V:1].[db].[bd'] .[gb].[bd'] .[gb] z::[V:3].[G,B,].[G,B,] .[G,B,].[G,B,] .[G,B,] z::\n",
      "[r:20/60][V:1](G/B/d/B/) (G/B/d/B/) (G/B/d/B/)|[V:3][K:bass] D4{E} (DC/B,/)|[V:4][K:bass] x6|\n",
      "[r:21/59][V:1](^F/A/d/A/) (F/A/d/A/) (F/A/d/A/)|[V:3](C>A,) ^F,2 z2|\n",
      "[r:22/58][V:1](G/B/d/B/) (G/B/d/B/) (G/B/d/B/)|[V:3]D2- D/E/^F/G/ F/E/D/C/)|\n",
      "[r:23/57][V:1](^F/A/d/A/) (F/A/d/A/) (F/A/d/A/)|[V:3](C>A,) ^F,2 z2|\n",
      "[r:24/56][V:1]z/ (G/A/_B/ =B/c/^c/d/ _e/=e/f/^f/)|[V:3][B,D]2 z2 z2|\n",
      "[r:25/55][V:1](g/a/g/^f/) (g/a/b/c'/) (b/g/f/e/)|[V:3][K:treble] z2 [EG]2 z2|[V:4][K:treble] x6|\n",
      "[r:26/54][V:1](d/B/)(g/d/) d2 (Tc3/2B/4c/4)|[V:3](D/B/G/B/) (D/B/G/B/) (D/A/^F/A/)|\n",
      "[r:27/53][V:1]B2 z/ (g/a/b/ a/g/^f/e/)|[V:3](E/G/B/G/ E2) z2|\n",
      "[r:28/52][V:1](d/B/)(g/d/) d2 (Tc3/2B/4c/4)|[V:3](D/B/G/B/) (D/B/G/B/) (D/A/^F/A/)|\n",
      "[r:29/51][V:1].B/(g/d/B/ .A/)(e/c/A/ .G/)(d/B/G/)|[V:3][G,G]2 z2 z2|\n",
      "[r:30/50][V:1]^F6|[V:3]z/ (_E/D/E/ D/E/D/E/ F/E/D/C/)|\n",
      "[r:31/49][V:1].G/(_e/d/e/ d/e/d/e/ f/e/d/c/)|[V:3]_B,6|\n",
      "[r:32/48][V:1](g3/2a/4g/4) (^fe d^c)|[V:3][_B,^C]6|\n",
      "[r:33/47][V:1](d/B/d/g/ b/g/d'/b/ c'/a/g/^f/)|[V:3][B,D]2 z2 [Dc]2|\n",
      "[r:34/46][V:1](g/d/B/d/ g/e/d/B/ c/A/G/^F/)|[V:3][GB]2 z2 [D,C]2|\n",
      "[r:35/45][V:1](^F4 G) z|[V:3](C4 B,) z|[V:4]G,4- G, x|\n",
      "[r:36/44][V:1]\"^Trio\"!p! g4{a} (gf/e/|[V:3][K:bass] (C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|[V:4][K:bass] x6|\n",
      "[r:37/43][V:1].f2) z2 z2|[V:3](B,/G/D/G/) (B,/G/D/G/) (B,/G/D/G/)|\n",
      "[r:38/42][V:1]f3 (g/f/ e/d/c/B/)|[V:3](B,/G/D/G/) (B,/G/D/G/) (B,/G/D/G/)|\n",
      "[r:39/41][V:1](d2 .e2) z2|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\n",
      "[r:40/40][V:1](e/d/e/f/ ^c/d/e/d/ g/f/e/d/)|[V:3][F,A,]6|\n",
      "[r:41/39][V:1](d/c/d/e/ B/c/d/c/ f/e/d/c/)|[V:3][E,G,]6|\n",
      "[r:42/38][V:1](B/c/^G/A/ =G/F/E/D/ ^C/D/F/D/)|[V:3][F,A,]6|\n",
      "[r:43/37][V:1](C4 B,) z|[V:3]G,6|\n",
      "[r:44/36][V:1]z (G ^FG ce)|[V:3][K:treble] (C/E/) z z2 z2|[V:4][K:treble] x6|\n",
      "[r:45/35][V:1](g>e) .c2 z2|[V:3](C/E/G/E/) (C/E/G/E/) (C/E/G/E/)|\n",
      "[r:46/34][V:1](^c/d/f/d/) (=c2 B/c/e/c/)|[V:3](B,/D/G/D/) (C/E/G/E/) (C/E/G/E/)|\n",
      "[r:47/33][V:1](e2 .d2) z2|[V:3](G,/C/E/C/) (G,/B,/D/B,/) (G,/B,/D/B,/)|\n",
      "[r:48/32][V:1]z/ (G/A/B/ c/d/e/^f/ g/a/b/c'/)|[V:3]B,6|\n",
      "[r:49/31][V:1](^g/a/c'/e'/ =g>)(g b/a/c'/a/)|[V:3]([Bd]2 [ce]2) [^Fd]2|\n",
      "[r:50/30][V:1](a2 g)(d ^c=c)|[V:3](c2 B) (B ^A=A)|[V:4]G3 x3|\n",
      "[r:51/29][V:1]B z z2 z2|[V:3](G/^F/G/A/ B/c/d/c/ B/A/G/F/)|\n",
      "[r:52/28][V:1]z/ (g/a/b/ c'/d'/e'/^f'/ g'/f'/g'/e'/)|[V:3]E6|\n",
      "[r:53/27][V:1](d'2 ^c'/)(g'/e'/c'/ d'/)(b/=c'/a/)|[V:3]z/ (E/G/^A/) [D=A]2 z [^Fd]|\n",
      "[r:54/26][V:1].g([ca]/4[Bg]/4[A^f]/4[Bg]/4) .[db]([ec']/4[db]/4[^c^a]/4[db]/4) .[gd']([^f=c']/4[e=c']/4[db]/4[ec']/4)|[V:3][GB] z [G,B,] z [G,C] z|\n",
      "[r:55/25][V:1].[db].[bd'] .[gb].[bd'] .[gb] z:|[V:3].[G,B,].[G,B,] .[G,B,].[G,B,] .[G,B,] z:|\n",
      "[r:56/24][V:1]!p! (c/d/e/f/ g/a/b/c'/) (b/c'/b/c'/)|[V:3][K:bass] C6|[V:4][K:bass] x6|\n",
      "[r:57/23][V:1]c6|[V:3](D/E/F/G/ A/B/c/d/) (^c/d/c/d/)|\n",
      "[r:58/22][V:1](d/e/f/g/ a/f/e/d/ c/B/A/B/)|[V:3][GB]4 [G,F]2|\n",
      "[r:59/21][V:1](c/G/e/d/ c/G/e/d/ c/G/e/d/)|[V:3][CE] z [CE] z [CE] z|\n",
      "[r:60/20][V:1](c/A/_B/G/ F/D/^C/E/ D/A,/_B,/^G,/)|[V:3][F,A,]2 z2 z2|\n",
      "[r:61/19][V:1]z/ (G,/C/E/) z/ (C/E/G/) z/ (D/F/B/)|[V:3]G,, z G,, z G,, z|\n",
      "[r:62/18][V:1](c/d/e/f/ g/a/b/c'/ b/c'/b/c'/)|[V:3][C,E,]2 z2 z2|\n",
      "[r:63/17][V:1]c6|[V:3][K:treble] (D/E/F/G/ A/B/c/d/) (^c/d/c/d/)|[V:4][K:treble] x6|\n",
      "[r:64/16][V:1](d/e/f/g/ a/f/e/d/ c/B/A/B/)|[V:3][GB]4 [G,F]2|\n",
      "[r:65/15][V:1](c/G/e/d/ c/G/e/d/ c/G/e/c/)|[V:3][CE] z [CE] z [CE] z|\n",
      "[r:66/14][V:1](_B/A/d/B/ A/F/d/A/ G/F/d/G/)|[V:3][DF]2 z2 z2|\n",
      "[r:67/13][V:1](F/E/_B/G/ F/E/B/G/ F/E/B/G/)|[V:3][K:bass] [C,C]6|[V:4][K:bass] x6|\n",
      "[r:68/12][V:1](A/_B/=B/c/ ^G/A/E/F/ ^C/D/F/A/)|[V:3][F,C]2 z2 z2|\n",
      "[r:69/11][V:1](C4 ED)|[V:2]x4 B,2|[V:3]z/ (G,/^F,/G,/ F,/G,/A,/G,/) G,,2|\n",
      "[r:70/10][V:1]C/(G/^F/G/ A/G/=F/E/ D/F/B,/D/)|[V:3]C,2 z2 ([G,,F,]2|\n",
      "[r:71/9][V:1]C/(g/^f/g/ a/g/=f/e/ d/f/B/d/)|[V:3][C,E,]2) z2 ([G,F]2|\n",
      "[r:72/8][V:1](c/g/^f/g/ a/g/=f/e/ d/f/B/d/)|[V:3][CE]2) z2[K:treble] ([G,F]2|[V:4]x4[K:treble] x2|\n",
      "[r:73/7][V:1](c/e/A/d/) (c/e/A/d/) (c/e/A/d/)|[V:3][CE])([G,F] [CE])([G,F] [CE])([G,F]|\n",
      "[r:74/6][V:1]c z z/ (C/D/E/ F/G/A/B/)|[V:3][CE]/)[K:bass](C,/E,/G,/ C2) z2|[V:4]x/[K:bass] x11/2|\n",
      "[r:75/5][V:1].c2 z/ (G/A/B/ c/d/e/f/)|[V:3]z/ (C,/E,/G,/ C2) z2|\n",
      "[r:76/4][V:1].g2 z/ (c/d/e/ f/g/a/b/)|[V:3]z/[K:treble] (C/E/G/ c2) z2|[V:4]x/[K:treble] x11/2|\n",
      "[r:77/3][V:1]c'2 z2 [ec']2|[V:3][K:bass] z/ (C,/E,/G,/ C/E/G/E/) (C/G,/E,/G,/)|[V:4][K:bass] x6|\n",
      "[r:78/2][V:1][ec']2 z2 [Ec]2|[V:3](C,/E,/G,/C/ E/G/E/C/) (G,/E,/C,/E,/)|\n",
      "[r:79/1][V:1][Ec]2 [E,C]2 [E,C]2|[V:3]C,2 C,,2 C,,2|\n",
      "[r:80/0][V:1][E,C]6|][V:3]C,,6|]\n",
      "-- decoded from utf-8\n",
      "-- skipped header: (field X,1)\n",
      "./fine_tuned_Classical_Beethoven_Ludwig_van_Keyboard_1.xml written\n",
      "-- done in 0.40 secs\n",
      "%Classical\n",
      "%Beethoven, Ludwig van\n",
      "%Keyboard\n",
      "%%score { ( 1 2 ) | ( 3 4 ) } \n",
      "L:1/8\n",
      "Q:1/4=60\n",
      "M:3/4\n",
      "K:C\n",
      "V:1 treble nm=\"piano\"\n",
      "V:2 treble\n",
      "V:3 bass nm=\"piano\"\n",
      "V:4 bass\n",
      "[r:0/80][V:1]\"^Andante con moto\"!p! g4{a} (gf/e/)|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\n",
      "[r:1/79][V:1]^c2 d2) z2|[V:3](B,/G/F/G/) (B,/G/F/G/) (B,/G/F/G/)|\n",
      "[r:2/78][V:1]d3 (.d.e.f)|[V:3](B,/G/D/G/) (B,/G/D/G/) (B,/G/D/G/)|\n",
      "[r:3/77][V:1](^d2 e2) z2|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\n",
      "[r:4/76][V:1](c'3 b/a/ g/f/e/d/)|[V:3](E,/C/G,/C/) (E,/C/G,/C/) (E,/C/G,/C/)|\n",
      "[r:5/75][V:1](^cd fa) (g/f/e/d/)|[V:3](F,/D/A,/D/) (F,/D/A,/D/) (F,/D/A,/D/)|\n",
      "[r:6/74][V:1](c3 d/4c/4B/4c/4 ed)|[V:3](G,/E/C/E/) (G,/E/C/E/) (G,/F/B,/F/)|\n",
      "[r:7/73][V:1]([FB]2 [Ec]2) z2|[V:3](C,/C/G,/E,/) C,2 z2|\n",
      "[r:8/72][V:1]!p! (g2- g/^f/a/g/) (=f/e/d/c/)|[V:3][K:treble] (C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|[V:4][K:treble] x6|\n",
      "[r:9/71][V:1](^A2 B2) z2|[V:3](D/G/F/G/) (D/G/F/G/) (D/G/F/G/)|\n",
      "[r:10/70][V:1](g2- g/^f/a/g/) (g/=f/e/^d/)|[V:3](B,/G/D/G/) (B,/G/D/G/) (B,/G/D/G/)|\n",
      "[r:11/69][V:1](^d2 e2) z2|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\n",
      "[r:12/68][V:1]!f! (c'3 d'/4c'/4b/4c'/4 e'g)|[V:3][K:bass] (E,/C/G,/C/) (E,/C/G,/C/) (E,/C/G,/C/)|[V:4][K:bass] x6|\n",
      "[r:13/67][V:1](a3 b/4a/4^g/4a/4) (d'/c'/b/a/)|[V:3](F,/D/A,/D/) (F,/D/A,/D/) (F,/D/A,/D/)|\n",
      "[r:14/66][V:1](g3 ^f/4g/4a/4g/4) (g/=f/e/d/)|[V:3](G,/E/C/E/) (G,/E/C/E/) (G,/F/B,/F/)|\n",
      "[r:15/65][V:1]c (G2 ^F/G/ A/G/c/B/)|[V:3][CE] z z2 z2|\n",
      "[r:16/64][V:1](d3 e/d/ g/^f/e/d/)|[V:3][K:treble] (B,/G/D/G/) (B,/G/D/G/) (B,/G/D/G/)|[V:4][K:treble] x6|\n",
      "[r:17/63][V:1](d/c/)(e/d/) d2 z2|[V:3](A,/^F/C/F/) (A,/F/C/F/) (A,/F/C/F/)|\n",
      "[r:18/62][V:1](d3 e/d/ g/^f/e/d/)|[V:3](B,/G/D/G/) (B,/G/D/G/) (B,/G/D/G/)|\n",
      "[r:19/61][V:1](d/c/)(e/d/) d2 z2|[V:3](A,/^F/C/F/) (A,/F/C/F/) (A,/F/C/F/)|\n",
      "[r:20/60][V:1]!f! (g/a/g/^f/ g/a/b/c'/ d'/b/a/g/)|[V:3]B,2 z2[K:bass] [B,D]2|[V:4]x4[K:bass] x2|\n",
      "[r:21/59][V:1](^f/e/^d/e/ d/e/d/e/ g/e/=d/c/)|[V:3]C2 z2 [C,C]2|\n",
      "[r:22/58][V:1](B4 !turn!dc)|[V:2]x4 ^F2|[V:3](D,/D/^C/D/ C/D/C/D/) (D,/=C/A,/C/)|\n",
      "[r:23/57][V:1][GB]!p! (.[Gd]2 .[Gd]2 .[Gd])|[V:3]([G,B,]2 [A,C]2 [B,D]2)|\n",
      "[r:24/56][V:1]!f! (g/a/g/^f/ g/a/b/c'/ d'/b/a/g/)|[V:3][B,D]2 z2 [B,D]2|\n",
      "[r:25/55][V:1](^f/e/^d/e/ d/e/d/e/ g/e/=d/c/)|[V:3]C2 z2 [C,C]2|\n",
      "[r:26/54][V:1](B3 c/4B/4A/4B/4 dc)|[V:2]x4 ^F2|[V:3](D,/D/^C/D/ C/D/C/D/) (D,/=C/A,/C/)|\n",
      "[r:27/53][V:1]!p! ([DGB]4 [C^FA]2)|[V:3](G,,/A,,/B,,/C,/ D,/E,/^F,/G,/ ^C,/D,/E,/F,/)|\n",
      "[r:28/52][V:1]([B,DG]4 [C^FA]2)|[V:3](G,,/A,,/B,,/C,/ D,/E,/^F,/G,/ ^C,/D,/E,/F,/)|\n",
      "[r:29/51][V:1][B,DG]2!pp! [GBd]2 [GBd]2|[V:3]G,,2 [G,,G,]2 [G,,G,]2|\n",
      "[r:30/50][V:1][GBd]2 z2 z2::[V:3][G,,G,]2 z2 z2::\n",
      "[r:31/49][V:1]!f! d4 (_e/d/c/_B/)|[V:3](G,/D/_B,/D/) (G,/D/B,/D/) (G,/D/B,/D/)|\n",
      "[r:32/48][V:1](^G2 A2) z2|[V:3](^F,/D/C/D/) (F,/D/C/D/) (F,/D/C/D/)|\n",
      "[r:33/47][V:1]A3 (.A.d.c)|[V:3](^F,/D/C/D/) (F,/D/C/D/) (F,/D/A,/D/)|\n",
      "[r:34/46][V:1](^A2 B2) z2|[V:3](G,/D/_B,/D/) (G,/D/B,/D/) (G,/D/B,/D/)|\n",
      "[r:35/45][V:1]!f! _e3 (gfe)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m voices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mV:1 treble nm=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpiano\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"V:2 treble\\n\"\"\"\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"V:3 bass nm=\"piano\"\\n\"\"\"\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"V:4 bass\\n\"\"\"\u001b[39;00m]\n\u001b[1;32m     15\u001b[0m first_tune \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m[r:0/80][V:1]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^Andante con moto\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!p! g4\u001b[39m\u001b[38;5;132;01m{a}\u001b[39;00m\u001b[38;5;124m (gf/e/)|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43minference_patch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomposer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstrumentation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%%score \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mscore\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdefault_note\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQ:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtempo\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmeter\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mK:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvoices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfirst_tune\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m abc_lines \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m     19\u001b[0m abc_lines \u001b[38;5;241m=\u001b[39m [line \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m abc_lines \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()]  \u001b[38;5;66;03m# Add newlines and remove empty lines\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 213\u001b[0m, in \u001b[0;36minference_patch\u001b[0;34m(period, composer, instrumentation, additional_prompts, tune_body)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[0;32m--> 213\u001b[0m         predicted_patch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_patches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOP_K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOP_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tunebody_flag \u001b[38;5;129;01mand\u001b[39;00m patchilizer\u001b[38;5;241m.\u001b[39mdecode([predicted_patch])\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[r:\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# 初次进入tunebody，必须以[r:0/开头\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         tunebody_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 381\u001b[0m, in \u001b[0;36mNotaGenLMHeadModel.generate\u001b[0;34m(self, patches, top_k, top_p, temperature)\u001b[0m\n\u001b[1;32m    378\u001b[0m generated_patch \u001b[38;5;241m=\u001b[39m []            \n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchar_level_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_patches\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# [128]\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     prob \u001b[38;5;241m=\u001b[39m top_k_sampling(prob, top_k\u001b[38;5;241m=\u001b[39mtop_k, return_probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# [128]\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     prob \u001b[38;5;241m=\u001b[39m top_p_sampling(prob, top_p\u001b[38;5;241m=\u001b[39mtop_p, return_probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# [128]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 313\u001b[0m, in \u001b[0;36mCharLevelDecoder.generate\u001b[0;34m(self, encoded_patch, tokens)\u001b[0m\n\u001b[1;32m    310\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((encoded_patch, tokens[:,\u001b[38;5;241m1\u001b[39m:,:]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Get output from model\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Get probabilities of next token\u001b[39;00m\n\u001b[1;32m    316\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1305\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1305\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1119\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         output_attentions,\n\u001b[1;32m   1117\u001b[0m     )\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:617\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    615\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    616\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 617\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    626\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:347\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    345\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    350\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/miniforge3/envs/notagen/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:217\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    214\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def sanitize(name: str) -> str:\n",
    "    \"\"\"Convert to safe filename (remove spaces, commas, etc.).\"\"\"\n",
    "    return re.sub(r'[^A-Za-z0-9]+', '_', name).strip('_')\n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        period, composer, instrumentation = 'Classical', 'Beethoven, Ludwig van', 'Keyboard'\n",
    "        default_note, tempo, meter, key = \"1/8\", \"1/4=60\", \"3/4\", \"C\"\n",
    "        # score = \"1 { ( 2 4 ) | ( 3 5 ) }\"\n",
    "        score = \"{ ( 1 2 ) | ( 3 4 ) }\"\n",
    "        voices = [\"\"\"V:1 treble nm=\"piano\"\\n\"\"\",\n",
    "                    \"\"\"V:2 treble\\n\"\"\",\n",
    "                    \"\"\"V:3 bass nm=\"piano\"\\n\"\"\",\n",
    "                    \"\"\"V:4 bass\\n\"\"\"]\n",
    "        first_tune = \"\"\"[r:0/80][V:1]\"^Andante con moto\"!p! g4{a} (gf/e/)|[V:3](C/G/E/G/) (C/G/E/G/) (C/G/E/G/)|\\n\"\"\"\n",
    "        result = inference_patch(period, composer, instrumentation, [f\"%%score {score} \\n\", f\"L:{default_note}\\n\", f\"Q:{tempo}\\n\", f\"M:{meter}\\n\", f\"K:{key}\\n\"]+ voices, [first_tune])\n",
    "\n",
    "        abc_lines = result.splitlines()\n",
    "        abc_lines = [line + '\\n' for line in abc_lines if line.strip()]  # Add newlines and remove empty lines\n",
    "\n",
    "        abc_lines = rest_unreduce(abc_lines)\n",
    "\n",
    "        # Construct unique file name\n",
    "        filename = f\"fine_tuned_{sanitize(period)}_{sanitize(composer)}_{sanitize(instrumentation)}_{i}.abc\"\n",
    "\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(abc_lines)\n",
    "        !python abc2xml.py -o . {filename}\n",
    "    except Exception as e:\n",
    "        print(f\"Exception caught {e}, skipping {sanitize(period)}_{sanitize(composer)}_{sanitize(instrumentation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2b601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26637bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%Classical\n",
      "%Beethoven, Ludwig van\n",
      "%Art Song\n",
      "%%score 1 { ( 2 4 ) | ( 3 5 ) } \n",
      "L:1/8\n",
      "Q:1/4=80\n",
      "M:3/4\n",
      "K:D\n",
      "V:1 treble nm=\"Singstimme\\nVoice\"\n",
      "V:2 treble nm=\"Pianoforte\"\n",
      "V:4 treble \n",
      "V:3 bass \n",
      "V:5 bass \n",
      "[r:0/37][V:1]\"^Etwas langsam\" z6|[V:3]!pp!{^G,,} [D,,A,,]6|\n",
      "[r:1/36][V:3]{^G,,} [D,,A,,]6|\n",
      "[r:2/35][V:2](D/!pp!F/A/F/ GEFD)|[V:3][D,,A,,]6|\n",
      "[r:3/34][V:2](A,/C/E/G/ FDEC)|[V:3][D,,A,,]6|\n",
      "[r:4/33][V:2](A,/D/F/A/ [EG][DF][CE][DF])|[V:3][D,,A,,]6|\n",
      "[r:5/32][V:2]([EG][CA]) ([DF]>[A,E] [F,D]2)|[V:3][D,,A,,]6|\n",
      "[r:6/31][V:2][A,E]6|[V:3][D,,A,,]6|\n",
      "[r:7/30][V:2][A,F]6|:[V:3][D,,A,,]6|:\n",
      "[r:8/29][V:1]!pp! (DF) A d A F|[V:2][A,D]6|[V:3][D,,A,,]6|\n",
      "[r:9/28][V:1](A3/2 G/) E2 F2|[V:2](E4 F2)|[V:3][D,,A,,]6|[V:4]C4 D2|\n",
      "[r:10/27][V:1](A/G/) (F/G/) E2 z F|[V:2]([DG]2 [CE]4)|[V:3](D,2 E,4)|[V:5]A,,6|\n",
      "[r:11/26][V:1]!<(! G A!<)!!>(! B e (c/B/) A!>)!|[V:2][A,E]4 [A,E]2|[V:3][D,,A,,]4 [D,,A,,]2|\n",
      "[r:12/25][V:1](A/G/F/G/) F2 z2|[V:2]([A,CE]2 [A,DF])(.[DF].[EG].[FA])|[V:3][D,,A,,]4 z2|\n",
      "[r:13/24][V:1]!mf! (DF) A d A F|[V:2][FAd]6|[V:3][D,,A,,]6|\n",
      "[r:14/23][V:1]!>(! (A>G) E2!>)! F2|[V:2](e4 f2)|[V:3][D,,A,,]6|[V:4]c4 d2|\n",
      "[r:15/22][V:1]!p! (A/G/) (F/G/) E2 z A|[V:2]([dg]2 [ce]4)|[V:3](D,2 E,4)|[V:5]A,,6|\n",
      "[r:16/21][V:1]!<(! A d!<)!!>(! f3/2 e/ (d/c/)!>)! (B/A/)|[V:2]d4 (d2|[V:3][D,,A,,]4 [D,,A,,]2|[V:4](FAFA) F2|\n",
      "[r:17/20][V:1](G/F/) (E/F/) D2 z2|[V:2]c2 d)(AGF)|[V:3][D,,A,,]4 [D,,A,,]2|[V:4](EG F2) z2|\n",
      "[r:18/19][V:2](E2 D4)|[V:3](G,2 F,4)|[V:5][D,,A,,]6|\n",
      "[r:19/18][V:1]!pp! (EF) G B A E|[V:2][A,E]6|[V:3][D,,A,,]6|\n",
      "[r:20/17][V:1](F>G) E2 A2|[V:2]([DF]2 [CE]4)|[V:3][D,,A,,]6|\n",
      "[r:21/16][V:1]!<(! (ce)!<)!!>(! g3/2 e/ (d/c/) (B/A/)!>)!|[V:2][A,E]4 [A,E]2|[V:3][D,,A,,]4 [D,,A,,]2|\n",
      "[r:22/15][V:1](G/F/) (E/F/) D2 z2|[V:2]([A,CE]2 [A,DF])(.[DF].[EG].[FA])|[V:3][D,,A,,]4 z2|\n",
      "[r:23/14][V:2]([Gc]2 [Fd]4)|[V:3][D,,A,,]6|\n",
      "[r:24/13][V:1]!pp! (EF) G B A E|[V:2][Ae]6|[V:3][D,,A,,]6|\n",
      "[r:25/12][V:1](F>G) E2 A2|[V:2](f2 e4)|[V:3][D,,A,,]6|[V:4]d2 c4|\n",
      "[r:26/11][V:1]!<(! c e!<)!!>(! g3/2 e/ (d/c/) (B/A/)!>)!|[V:2][Ae]4 [Ae]2|[V:3][D,,A,,]4 [D,,A,,]2|\n",
      "[r:27/10][V:1](G/F/) (E/F/) D2 z2|[V:2]([Ae]2 [Fd])(FGA)|[V:3][D,,A,,]4 [D,,A,,]2|\n",
      "[r:28/9][V:2]!>(! ([EG]2 [DF]4)!>)!|[V:3][D,,A,,]6|\n",
      "[r:29/8][V:2]!pp! z6:|[V:3]{^G,,} [D,,A,,]6:|\n",
      "[r:30/7][V:1]!pp! (DF) A d A F|[V:2][A,D]6|[V:3][D,,A,,]6|\n",
      "[r:31/6][V:1](A>G) E2 F2|[V:2](E4 F2)|[V:3][D,,A,,]6|[V:4]C4 D2|\n",
      "[r:32/5][V:1](A/G/) (F/G/) E2 z F|[V:2]([DG]2 [CE]4)|[V:3](D,2 E,4)|[V:5]A,,6|\n",
      "[r:33/4][V:1]!<(! G A!<)!!>(! B e (c/B/)!>)! A|[V:2][A,E]4 [A,E]2|[V:3][D,,A,,]4 [D,,A,,]2|\n",
      "[r:34/3][V:1](A/G/F/G/) F2 z2|[V:2]([A,CE]2 [A,DF])(.[DF].[EG].[FA])|[V:3][D,,A,,]4 z2|\n",
      "[r:35/2][V:2]!>(! ([Gc]2 [Fd]4)|[V:3][D,,A,,]6|\n",
      "[r:36/1][V:1]!ppp!!>(! (A/B/) (A/^G/) A2!>)! z2|[V:2]([Ac]2 [Ad]4-)|[V:3][D,,A,,]4 [D,,A,,]2|\n",
      "[r:37/0][V:1]!fermata!z6|][V:2][Ad]4!>)! !fermata!z2|][V:3][D,,A,,]4 !fermata!z2|]\n",
      "-- decoded from utf-8\n",
      "-- skipped header: (field X,1)\n",
      "./Classical_Beethoven_Ludwig_van_Art_Song_2.xml written\n",
      "-- done in 0.20 secs\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    period, composer, instrumentation = 'Classical', 'Beethoven, Ludwig van', 'Art Song'\n",
    "    default_note, tempo, meter, key = \"1/8\", \"1/4=80\", \"3/4\", \"D\"\n",
    "    score = \"1 { ( 2 4 ) | ( 3 5 ) }\"\n",
    "    # score = \"[ 1 2 3 4 ]\"\n",
    "    # score = \"\"\n",
    "    # print([f\"%%score {score} \\n\", f\"L:{default_note}\\n\", f\"Q:{tempo}\\n\", f\"M:{meter}\\n\", f\"K:{key}\\n\"])\n",
    "    result = inference_patch(period, composer, instrumentation, [f\"%%score {score} \\n\", \n",
    "    f\"L:{default_note}\\n\", \n",
    "    f\"Q:{tempo}\\n\", \n",
    "    f\"M:{meter}\\n\", \n",
    "    f\"K:{key}\\n\"])\n",
    "\n",
    "    abc_lines = result.splitlines()\n",
    "    abc_lines = [line + '\\n' for line in abc_lines if line.strip()]  # Add newlines and remove empty lines\n",
    "\n",
    "    abc_lines = rest_unreduce(abc_lines)\n",
    "\n",
    "    # Construct unique file name\n",
    "    filename = f\"{sanitize(period)}_{sanitize(composer)}_{sanitize(instrumentation)}_{i}.abc\"\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(abc_lines)\n",
    "    !python abc2xml.py -o . {filename}\n",
    "except Exception as e:\n",
    "    print(f\"Exception caught {e}, skipping {sanitize(period)}_{sanitize(composer)}_{sanitize(instrumentation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65f9ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%Classical\n",
      "%Haydn, Joseph\n",
      "%Chamber\n",
      "%%score 1 { ( 2 4 ) | ( 3 5 ) } \n",
      "L:1/8\n",
      "Q:1/4=60\n",
      "M:3/4\n",
      "K:C\n",
      "V:1 treble nm=\"Violino\"\n",
      "V:2 treble nm=\"\"\n",
      "V:4 treble\n",
      "V:3 bass\n",
      "V:5 bass\n",
      "[r:0/101][V:1]\"^Andante cantabile\" (g3/2a/4b/4) (c'/g/e/c/) (a/f/d/c/)|[V:2][EGc]4 [FAc]2|[V:3]C,2 E,2 F,2|\n",
      "[r:1/100][V:1](Tc3/2B/4c/4) B2 z2|[V:2][EGc]2 [DGB]2 [GBd]2|[V:3]G,2 G,,G,D,B,,|\n",
      "[r:2/99][V:1](f3/2g/4a/4) (g/f/e/d/) (c/B/A/G/)|[V:2][Gdf]4 [GBd]2|[V:3]G,,2 G,2 F,2|\n",
      "[r:3/98][V:1](c>d) (Td3/2c/4d/4) e z|[V:2][Gc]2 [DGB]2 [EGc]2|[V:3]E,2 G,2 C2|\n",
      "[r:4/97][V:1](f3/2g/4a/4) (g/f/e/d/) (c/B/A/G/)|[V:2][FGd]4 [DGB]2|[V:3]B,2 B,,2 F,2|\n",
      "[r:5/96][V:1](c>d) (Td3/2c/4d/4) e z|[V:2][CGc]2 [FBd]2 [EGc]2|[V:3]E,2 G,2 C2|\n",
      "[r:6/95][V:1](^g/a/=g/f/) (e/d/^c/d/) (c/d/f/d/)|[V:2][GAe]2 [FAd]4|[V:3]F,2 F,,2 F,2|\n",
      "[r:7/94][V:1](c2 B2) z2|[V:2][EGc]2 [DGB]2 [GBd]2|[V:3]G,2 G,,G,D,B,,|\n",
      "[r:8/93][V:1](d3/2e/4^f/4) (g/d/B/G/) (e/c/B/A/)|[V:2][Gdg]4 [Gce]2|[V:3]G,,2 B,,2 C,2|\n",
      "[r:9/92][V:1](TG3/2^F/4G/4) F2 z2|[V:2][GBd]2 [^FAd]2 [Ad^f]2|[V:3]D,2 D,,D,A,,^F,,|\n",
      "[r:10/91][V:1](c3/2d/4e/4) (d/c/B/A/) (G/^F/E/D/)|[V:2][cda]4 [Ad^f]2|[V:3]D,,2 D,2 C,2|\n",
      "[r:11/90][V:1](G>A) (TA3/2G/4A/4) B z|[V:2][Gdg]2 [Ad^f]2 [Bdg]2|[V:3]B,,2 D,2 G,2|\n",
      "[r:12/89][V:1](c3/2d/4e/4) (d/c/B/A/) (G/^F/E/D/)|[V:2][cda]4 [Ad^f]2|[V:3]^F,2 ^F,,2 C,2|\n",
      "[r:13/88][V:1](G/B/d/g/) (b/d'/c'/b/) (a/g/^f/g/)|[V:2][Gdg]2 [Gd]4|[V:3]B,,2 B,2 B,,2|\n",
      "[r:14/87][V:1](^f/e/^d/e/) (=d/c/B/c/) (B/A/^G/A/)|[V:2][^Fc]2 [GAe]4|[V:3]A,,2 C,2 ^C,2|\n",
      "[r:15/86][V:1](G2 ^F2) z2|[V:2][GBd]2 [^FAd]2 [FAd]2|[V:3]D,2 D,,D,A,,^F,,|\n",
      "[r:16/85][V:1](a2- a/b/c'/b/) (d'/c'/b/a/)|[V:2][DAd]4 [^FAd]2|[V:3]D,,2 D,2 C,2|\n",
      "[r:17/84][V:1](g3 ^f/g/ a/g/f/g/)|[V:2][Gd]2 [GBd]2 [Gdg]2|[V:3]B,,2 G,,2 B,,2|\n",
      "[r:18/83][V:1](^f/e/^d/e/) e3 c'|[V:2][Gd^f]2 [Gce]2 [Acd]2|[V:3]C,2 E,2 ^F,2|\n",
      "[r:19/82][V:1](Tc'3/2b/4c'/4) b2 z2|[V:2][Acd]2 [GBd]2 [Gdg]2|[V:3]G,2 G,,G,B,,G,|\n",
      "[r:20/81][V:1](a2- a/b/c'/b/) (d'/c'/b/a/)|[V:2][Aeg]2 [Ad^f]2 [Acd]2|[V:3]C,2 D,2 ^F,2|\n",
      "[r:21/80][V:1](g3 ^f/g/ a/g/f/g/)|[V:2][GBd]2 [Gdg]4|[V:3]G,2 B,,2 B,2|\n",
      "[r:22/79][V:1](e/a/c'/a/) g2 T^f2|[V:2][Gce]2 [GBd]2 [^FAd]2|[V:3]C2 D2 D,2|\n",
      "[r:23/78][V:1]g2 z/ (g/^f/g/) (b/g/d/B/)|[V:2][GBd]4 [GBd]2|[V:3]G,2 G,,2 G,2|\n",
      "[r:24/77][V:1](Tc>B) c/(^f/e/f/) (a/f/c/A/)|[V:2][Gce]2 [^Fcd]4|[V:3]A,2 D,2 D2|\n",
      "[r:25/76][V:1](TB>A) B/(g/^f/g/) (b/g/d/B/)|[V:2][GBd]4 [GBd]2|[V:3]G,2 G,,2 G,2|\n",
      "[r:26/75][V:1](Tc>B) c/(^f/e/f/) (a/f/c/A/)|[V:2][Gce]2 [^Fcd]4|[V:3]A,2 D,2 D2|\n",
      "[r:27/74][V:1]G2 [G,DBg]2 z2::[V:2][GBd]4 z2::[V:3]G,2 G,,2 z2::\n",
      "[r:28/73][V:1](G3/2A/4B/4) (c/B/A/G/) (c/d/e/^f/)|[V:2][DGB]4 [EGc]2|[V:3]G,,2 G,2 G,2|\n",
      "[r:29/72][V:1](g3/2a/4b/4) (c'/b/a/g/) (c'/d'/e'/^f'/)|[V:2][GBd]4 [Gce]2|[V:3]G,2 G,2 G,2|\n",
      "[r:30/71][V:1]g'2 [G,DB]2 z2|[V:2][GBd]4 z2|[V:3]G,2 G,,2 z2|\n",
      "[r:31/70][V:1](g3/2a/4b/4) (c'/b/g/e/) (a/f/d/c/)|[V:2][EGc]4 [FAc]2|[V:3]C,2 E,2 F,2|\n",
      "[r:32/69][V:1](Tc3/2B/4c/4) B2 z2|[V:2][EGc]2 [DGB]2 [GBd]2|[V:3]G,2 G,,G,D,B,,|\n",
      "[r:33/68][V:1](f3/2g/4a/4) (g/f/e/d/) (c/B/A/G/)|[V:2][Gdf]4 [GBd]2|[V:3]G,,2 G,2 F,2|\n",
      "[r:34/67][V:1](c>d) (Td3/2c/4d/4) e z|[V:2][Gc]2 [DGB]2 [EGc]2|[V:3]E,2 G,2 C2|\n",
      "[r:35/66][V:1](f3/2g/4a/4) (g/f/e/d/) (c/B/A/G/)|[V:2][FGd]4 [DGB]2|[V:3]B,2 B,,2 F,2|\n",
      "[r:36/65][V:1](c>d) (Td3/2c/4d/4) e z|[V:2][CGc]2 [FBd]2 [EGc]2|[V:3]E,2 G,2 C2|\n",
      "[r:37/64][V:1](^g/a/=g/f/) (e/d/^c/d/) (c/d/f/d/)|[V:2][GAe]2 [FAd]4|[V:3]F,2 F,,2 F,2|\n",
      "[r:38/63][V:1](c2 B2) z2|[V:2][EGc]2 [DGB]2 [GBd]2|[V:3]G,2 G,,G,D,B,,|\n",
      "[r:39/62][V:1](d3/2e/4^f/4) (g/d/B/G/) (e/c/B/A/)|[V:2][Gdg]4 [Gce]2|[V:3]G,,2 B,,2 C,2|\n",
      "[r:40/61][V:1](TG3/2^F/4G/4) F2 z2|[V:2][GBd]2 [^FAd]2 [Ad^f]2|[V:3]D,2 D,,D,A,,^F,,|\n",
      "[r:41/60][V:1](c3/2d/4e/4) (d/c/B/A/) (G/^F/E/D/)|[V:2][cda]4 [Ad^f]2|[V:3]D,,2 D,2 C,2|\n",
      "[r:42/59][V:1](G>A) (TA3/2G/4A/4) B z|[V:2][Gdg]2 [Ad^f]2 [Bdg]2|[V:3]B,,2 D,2 G,2|\n",
      "[r:43/58][V:1](c3/2d/4e/4) (d/c/B/A/) (G/^F/E/D/)|[V:2][cda]4 [Ad^f]2|[V:3]^F,2 ^F,,2 C,2|\n",
      "[r:44/57][V:1](G/B/d/g/) (b/d'/c'/b/) (a/g/^f/g/)|[V:2][Gdg]2 [Gd]4|[V:3]B,,2 B,2 B,,2|\n",
      "[r:45/56][V:1](^f/e/^d/e/) (=d/c/B/c/) (B/A/^G/A/)|[V:2][^Fc]2 [GAe]4|[V:3]A,,2 C,2 ^C,2|\n",
      "[r:46/55][V:1](G2 ^F2) z2|[V:2][GBd]2 [^FAd]2 [FAd]2|[V:3]D,2 D,,D,A,,^F,,|\n",
      "[r:47/54][V:1](a2- a/b/c'/b/) (d'/c'/b/a/)|[V:2][DAd]4 [^FAd]2|[V:3]D,,2 D,2 C,2|\n",
      "[r:48/53][V:1](g3 ^f/g/ a/g/f/g/)|[V:2][Gd]2 [GBd]2 [Gdg]2|[V:3]B,,2 G,,2 B,,2|\n",
      "[r:49/52][V:1](^f/e/^d/e/) e3 c'|[V:2][Gd^f]2 [Gce]2 [Acd]2|[V:3]C,2 E,2 ^F,2|\n",
      "[r:50/51][V:1](Tc'3/2b/4c'/4) b2 z2|[V:2][Acd]2 [GBd]2 [Gdg]2|[V:3]G,2 G,,G,B,,G,|\n",
      "[r:51/50][V:1](a2- a/b/c'/b/) (d'/c'/b/a/)|[V:2][Aeg]2 [Ad^f]2 [Acd]2|[V:3]C,2 D,2 ^F,2|\n",
      "[r:52/49][V:1](g3 ^f/g/ a/g/f/g/)|[V:2][GBd]2 [Gdg]4|[V:3]G,2 B,,2 B,2|\n",
      "[r:53/48][V:1](e/a/c'/a/) g2 T^f2|[V:2][Gce]2 [GBd]2 [^FAd]2|[V:3]C2 D2 D,2|\n",
      "[r:54/47][V:1]g2 z/ (g/^f/g/) (b/g/d/B/)|[V:2][GBd]4 [GBd]2|[V:3]G,2 G,,2 G,2|\n",
      "[r:55/46][V:1](Tc>B) c/(^f/e/f/) (a/f/c/A/)|[V:2][Gce]2 [^Fcd]4|[V:3]A,2 D,2 D2|\n",
      "[r:56/45][V:1](TB>A) B/(g/^f/g/) (b/g/d/B/)|[V:2][GBd]4 [GBd]2|[V:3]G,2 G,,2 G,2|\n",
      "[r:57/44][V:1](Tc>B) c/(^f/e/f/) (a/f/c/A/)|[V:2][Gce]2 [^Fcd]4|[V:3]A,2 D,2 D2|\n",
      "[r:58/43][V:1]G2 [G,DBg]2 z2::[V:2][GBd]4 z2::[V:3]G,2 G,,2 z2::\n",
      "[r:59/42][V:1][M:3/4][Q:1/4=90]\"^III. Andante\" (ge)(ec)(cB)|[V:2][M:3/4] [EGc]4 z2|[V:3][M:3/4] C,4 z2|[V:4][M:3/4] x6|[V:5][M:3/4] x6|\n",
      "[r:60/41][V:1](B4 c2)|[V:2](B4 c2)|[V:3](G,F,)(F,E,)(E,D,)|[V:4]D2 G4|\n",
      "[r:61/40][V:1](fd)(dB)(BA)|[V:2][FBd]4 z2|[V:3]D,4 z2|\n",
      "[r:62/39][V:1](A4 G2)|[V:2](A4 G2)|[V:3](F,E,)(E,D,)(D,C,)|[V:4]F2 D4|\n",
      "[r:63/38][V:1](ef) (g3 ^g)|[V:2][EGc]2 [CGc]4|[V:3]C,2 E,2 E,2|\n",
      "[r:64/37][V:1](ab) (c'3 ^c')|[V:2][CFA]2 [CFc]4|[V:3]F,2 A,2 A,,2|\n",
      "[r:65/36][V:1](d'e')(f'e'd'c')|[V:2][DFB]4 [DAd]2|[V:3]B,,2 D,2 F,2|\n",
      "[r:66/35][V:1](bagfed)|[V:2][DGB]4 [DGd]2|[V:3]G,2 G,,2 B,,2|\n",
      "[r:67/34][V:1](eg)(fe)(dc)|[V:2][EGc]2 [FAd]2 [DAc]2|[V:3]C,2 F,2 ^F,2|\n",
      "[r:68/33][V:1](BA) G2 z2|[V:2][DGB]4 z2|[V:3]G,3 A,G,F,|\n",
      "[r:69/32][V:1](ge)(ec)(cB)|[V:2][CGc]4 z2|[V:3]E,4 z2|\n",
      "[r:70/31][V:1](B4 c2)|[V:2](B4 c2)|[V:3](G,F,)(F,E,)(E,D,)|[V:4]D2 G4|\n",
      "[r:71/30][V:1](fd)(dB)(BA)|[V:2][FBd]4 z2|[V:3]D,4 z2|\n",
      "[r:72/29][V:1](A4 G2)|[V:2](A4 G2)|[V:3](F,E,)(E,D,)(D,C,)|[V:4]F2 D4|\n",
      "[r:73/28][V:1](ef) (g3 ^g)|[V:2][EGc]2 [CGc]4|[V:3]C,2 E,2 E,2|\n",
      "[r:74/27][V:1](ab) (c'3 ^c')|[V:2][CFA]2 [CFc]4|[V:3]F,2 A,2 A,,2|\n",
      "[r:75/26][V:1](d'f')(e'd'c'b)|[V:2][DFB]2 [CEA]2 [B,E^G]2|[V:3]D,2 E,2 E,,2|\n",
      "[r:76/25][V:1]a4 z2|[V:2][CEA]4 z2|[V:3]A,,2 A,2 G,2|\n",
      "[r:77/24][V:1](ab)(c'a)(g^f)|[V:2][DAd]4 [D^FA]2|[V:3]^F,4 C,2|\n",
      "[r:78/23][V:1](ga)(bg)(dB)|[V:2][DG]4 [DGd]2|[V:3]B,,4 B,2|\n",
      "[r:79/22][V:1](Ae)(dc)(BA)|[V:2][EGA]2 [D^FA]4|[V:3]C2 D2 D,2|\n",
      "[r:80/21][V:1](Bc) d2 z2|[V:2][DGB]4 [DGd]2|[V:3]G,3 A,G,F,|\n",
      "[r:81/20][V:1](cd)(ec)(BA)|[V:2][EGc]4 [DAd]2|[V:3]E,4 ^F,2|\n",
      "[r:82/19][V:1](Bc)(dB)(AG)|[V:2][DGB]4 [CGc]2|[V:3]G,4 E,2|\n",
      "[r:83/18][V:1](AG)(FE)(DC)|[V:2][CFA]2 [DAd]4|[V:3]F,2 ^F,4|\n",
      "[r:84/17][V:1]B,2 z2 z2|[V:2][DGB]4 z2|[V:3]G,3 A,G,F,|\n",
      "[r:85/16][V:1](ge)(ec)(cB)|[V:2][CGc]4 z2|[V:3]E,4 z2|\n",
      "[r:86/15][V:1](B4 c2)|[V:2](B4 c2)|[V:3](G,F,)(F,E,)(E,D,)|[V:4]D2 G4|\n",
      "[r:87/14][V:1](fd)(dB)(BA)|[V:2][FBd]4 z2|[V:3]D,4 z2|\n",
      "[r:88/13][V:1](A4 G2)|[V:2](A4 G2)|[V:3](F,E,)(E,D,)(D,C,)|[V:4]F2 D4|\n",
      "[r:89/12][V:1](ef) (g3 ^g)|[V:2][EGc]2 [CGc]4|[V:3]C,2 E,2 E,2|\n",
      "[r:90/11][V:1](ab) (c'3 ^c')|[V:2][CFA]2 [CFc]4|[V:3]F,2 A,2 A,,2|\n",
      "[r:91/10][V:1](d'f')(e'd'c'b)|[V:2][DFB]2 [CEA]2 [B,E^G]2|[V:3]D,2 E,2 E,,2|\n",
      "[r:92/9][V:1]a4 z2|[V:2][CEA]4 z2|[V:3]A,,2 A,2 G,2|\n",
      "[r:93/8][V:1](ab)(c'a)(g^f)|[V:2][DAd]4 [D^FA]2|[V:3]^F,4 C,2|\n",
      "[r:94/7][V:1](ga)(bg)(dB)|[V:2][DG]4 [DGd]2|[V:3]B,,4 B,2|\n",
      "[r:95/6][V:1](Ae)(dc)(BA)|[V:2][EGA]2 [D^FA]4|[V:3]C2 D2 D,2|\n",
      "[r:96/5][V:1](Bc) d2 z2|[V:2][DGB]4 [DGd]2|[V:3]G,3 A,G,F,|\n",
      "[r:97/4][V:1](cd)(ec)(BA)|[V:2][EGc]4 [DAd]2|[V:3]E,4 ^F,2|\n",
      "[r:98/3][V:1](Bc)(dB)(AG)|[V:2][DGB]4 [CGc]2|[V:3]G,4 E,2|\n",
      "[r:99/2][V:1](AG)(FE)(DC)|1[V:2][CFA]2 [DAd]4|1[V:3]F,2 ^F,4|1\n",
      "[r:100/1][V:1]B,4 z2:|2[V:2][DGB]4 z2:|2[V:3]G,3 A,G,F,:|2\n",
      "[r:101/0][V:1][Q:1/4=60] B,4 !fermata!z2|][V:2][DGB]4 !fermata!z2|][V:3]G,4 !fermata!z2|]\n",
      "-- decoded from utf-8\n",
      "-- skipped header: (field X,1)\n",
      "./Classical_Haydn_Joseph_Chamber_0.xml written\n",
      "-- done in 0.58 secs\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    period, composer, instrumentation = 'Classical', 'Haydn, Joseph', 'Chamber'\n",
    "    default_note, tempo, meter, key = \"1/8\", \"1/4=60\", \"3/4\", \"C\"\n",
    "    score = \"1 { ( 2 4 ) | ( 3 5 ) }\"\n",
    "    # score = \"{ ( 1 2 ) | ( 3 4 ) }\"\n",
    "    result = inference_patch(period, composer, instrumentation, [f\"%%score {score} \\n\", f\"L:{default_note}\\n\", f\"Q:{tempo}\\n\", f\"M:{meter}\\n\", f\"K:{key}\\n\"])\n",
    "\n",
    "    abc_lines = result.splitlines()\n",
    "    abc_lines = [line + '\\n' for line in abc_lines if line.strip()]  # Add newlines and remove empty lines\n",
    "\n",
    "    abc_lines = rest_unreduce(abc_lines)\n",
    "\n",
    "    # Construct unique file name\n",
    "    filename = f\"{sanitize(period)}_{sanitize(composer)}_{sanitize(instrumentation)}_{i}.abc\"\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(abc_lines)\n",
    "    !python abc2xml.py -o . {filename}\n",
    "except Exception as e:\n",
    "    print(f\"Exception caught {e}, skipping {sanitize(period)}_{sanitize(composer)}_{sanitize(instrumentation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccc5e3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%Classical\n",
      "%Beethoven, Ludwig van\n",
      "%Keyboard\n",
      "%%score { ( 1 2 ) | ( 3 4 ) } \n",
      "L:1/8\n",
      "Q:1/4=60\n",
      "M:3/4\n",
      "K:C\n",
      "V:1 treble nm=\"piano\"\n",
      "V:2 treble \n",
      "V:3 bass nm=\"piano\"\n",
      "V:4 bass \n",
      "[r:0/135][V:1]\"^Andante cantabile\"!p! (c3 d/c/ B/c/e/c/)|[V:3]EGEGEG|[V:4]C2 C2 C2|\n",
      "[r:1/134][V:1](^A2 B2) z2|[V:3]FGFGFG|[V:4]D2 D2 D2|\n",
      "[r:2/133][V:1](g3 a/g/ ^f/g/a/=f/)|[V:3]DGDGDG|[V:4]B,2 B,2 B,2|\n",
      "[r:3/132][V:1](^d2 e2) z2|[V:3]EGEGEG|[V:4]C2 C2 C2|\n",
      "[r:4/131][V:1]!f! (g3/2a/4g/4 c').b.a.g|[V:3]EGEGEG|[V:4]C2 C2 C2|\n",
      "[r:5/130][V:1](g/^f/c'/b/) (b/a/)(a/g/) (g/f/)(f/e/)|[V:3]DGDGDG|[V:4]B,2 B,2 B,2|\n",
      "[r:6/129][V:1](e/d/^g/a/) =g3 b/a/|[V:3]EADBC^F|[V:4]C2 D2 D,2|\n",
      "[r:7/128][V:1]g2 z2 z2::[V:3]B,/G/^F/G/ D/G/B,/D/ G,2::\n",
      "[r:8/127][V:1]!p! (d3 e/d/ g/f/e/d/)|[V:3][K:treble] GBGBGB|[V:4][K:treble] F2 F2 F2|\n",
      "[r:9/126][V:1](d>^d e2) z2|[V:3]GcGcGc|[V:4]E2 E2 E2|\n",
      "[r:10/125][V:1](g3 a/g/ c'/_b/a/g/)|[V:3]GcGcGc|[V:4]E2 E2 E2|\n",
      "[r:11/124][V:1](g>^g a2) z2|[V:3]AcAcAc|[V:4]F2 F2 F2|\n",
      "[r:12/123][V:1]!f! (a3 b/c'/ d'/c'/b/a/)|[V:3]DADADA|[V:4]B,2 B,2 B,2|\n",
      "[r:13/122][V:1](a/g/^f/g/) (f/g/f/g/) (a/g/=f/e/)|[V:3]EGEGEG|[V:4]C2 C2 C2|\n",
      "[r:14/121][V:1](e/d/f/a/) c3 e/d/|[V:3][K:bass] A,DG,EG,F|[V:4][K:bass] F,2 G,2 G,,2|\n",
      "[r:15/120][V:1]c2 z2 z2:|[V:3]C,/C/B,/C/ G,/C/E,/G,/ C,2:|\n",
      "[r:16/119][V:1][M:3/4]!p!\"^Trio\" (G4 c2)|[V:2][M:3/4] x6|[V:3][M:3/4] (3(CED (3CB,A, (3G,F,E,)|[V:4][M:3/4] x6|\n",
      "[r:17/118][V:1](c2 B2) z2|[V:3](3(F,A,G, (3F,E,D, (3C,B,,A,,)|\n",
      "[r:18/117][V:1](B4 f2)|[V:3](3(G,,B,,A,, (3G,,A,,G,, (3F,,E,,D,,)|\n",
      "[r:19/116][V:1](f2 e2) z2|[V:3](3(C,,E,,G,, (3C,E,G, (3CB,C)|\n",
      "[r:20/115][V:1]!f! (g4 c'2)|[V:3][K:treble] (3(ced (3cBA (3GFE)|[V:4][K:treble] x6|\n",
      "[r:21/114][V:1](c'2 b2 _b2)|[V:3](3(D^FD (3GDG (3EGE)|\n",
      "[r:22/113][V:1](3(abc') c2 (e>d)|[V:3](3(FAd) G2 F2|[V:4]x2 G,4|\n",
      "[r:23/112][V:1]c2 z2 z2:|[V:3][K:bass] (3(CEG (3CEG C2):|[V:4][K:bass] x6:|\n",
      "[r:24/111][V:1]!p! (g2 (3g)(.^f.g (3.a.b.c')|[V:3][K:treble] (3(GBd) G2 z2|[V:4][K:treble] x6|\n",
      "[r:25/110][V:1]d'2- (3d'(.c'.b (3.a.g.^f)|[V:3](3(^FAd) F2 z2|\n",
      "[r:26/109][V:1]e2- (3e(.^d.e (3.g.^f.e)|[V:3](3(EGB) E2 z2|\n",
      "[r:27/108][V:1](d2 ^c2) z2|[V:3](3(A,EG) (3(A,EG) (3(A,EG)|\n",
      "[r:28/107][V:1]!f! d2- (3(de^f (3gab)|[V:3](3(A,D^F) A,2 z2|\n",
      "[r:29/106][V:1]a2- (3(a.b.a (3.g.^f.e)|[V:3][K:bass] (3(A,EG) A,2 z2|[V:4][K:bass] x6|\n",
      "[r:30/105][V:1](3(de^c) (3(dA^f) (3(gec)|[V:3][D^F]2 [A,F]2 [A,E]2|\n",
      "[r:31/104][V:1]d2 z2 z2|[V:3](3(D,^F,G, (3A,B,^C (3D=CA,)|\n",
      "[r:32/103][V:1]!p! (3z (d^c (3dg^f (3agd)|[V:3]B,2 z2 z2|\n",
      "[r:33/102][V:1]e2 z2 z2|[V:3](3(C,E,^F, (3G,A,B, (3CA,F,)|\n",
      "[r:34/101][V:1](3z (cB (3c^fe (3gfc)|[V:3]D,2 z2 z2|\n",
      "[r:35/100][V:1]d2 z2 z2|[V:3](3(B,,D,^F, (3G,A,^A, (3B,G,E,)|\n",
      "[r:36/99][V:1]!f! (3z (d^c (3d^fg (3afd)|[V:3]^F,,2 z2 z2|\n",
      "[r:37/98][V:1](3(b^ab (3g^fg (3ege)|[V:3]G,,2 z2 z2|\n",
      "[r:38/97][V:1](3(d^fd) (3(AdA) (3(G^cG)|[V:3]A,,2 A,,2 A,,2|\n",
      "[r:39/96][V:1](3(^FDE (3FGA (3B^cd)|[V:3]D,2 z2 z2|\n",
      "[r:40/95][V:1](e2 ^f2 g2)|[V:3](3(^CA,C) (3(DA,D) (3(EA,E)|\n",
      "[r:41/94][V:1](3(^fDE (3^FGA (3B^cd)|[V:3]D2 z2 z2|\n",
      "[r:42/93][V:1](e2 ^f2 g2)|[V:3](3(^CA,C) (3(DA,D) (3(EA,E)|\n",
      "[r:43/92][V:1](3(^fde (3fga (3b^c'd')|[V:3]D2 z2 z2|\n",
      "[r:44/91][V:1]!p! (3(.e'.e'.e') (3(.e'.e'.e') (3(.e'.e'.e')|[V:3][K:treble] (3(.[^GBd].[GBd].[GBd]) (3(.[GBd].[GBd].[GBd]) (3(.[GBd].[GBd].[GBd])|[V:4][K:treble] x6|\n",
      "[r:45/90][V:1](3(.e'.e'.e') (3(.e'.e'.e') (3(.e'.e'.e')|[V:3](3(.[A^c].[Ac].[Ac]) (3(.[GB].[GB].[GB]) (3(.[^FA].[FA].[FA])|\n",
      "[r:46/89][V:1](3(.e'.e'.e') (3(.e'.e'.e') (3(.e'.e'.e')|[V:3](3(.[E^G].[EG].[EG]) (3(.[D^F].[DF].[DF]) (3(.[^CE].[CE].[CE])|\n",
      "[r:47/88][V:1]!f! (3(.e'.e'.e') (3(.e'.e'.e') (3(.e'.e'.e')|[V:3][K:bass] (3(.[B,D].[B,D].[B,D]) (3(.[A,^C].[A,C].[A,C]) (3(.[^G,B,].[G,B,].[G,B,])|[V:4][K:bass] x6|\n",
      "[r:48/87][V:1](3(e'^f'e' (3d'^c'b (3a^g^f)|[V:3][^F,A,]2 z2 z2|\n",
      "[r:49/86][V:1](3(e^d=d (3^cBA (3^GAB)|\n",
      "[r:50/85][V:1](3(^cAe) d2 TB2|[V:3]z2 ^C2 D2|[V:4]x2 E,4|\n",
      "[r:51/84][V:1]A2 z2 z2|[V:3](3(A,^CD (3E^F^G (3AGA)|\n",
      "[r:52/83][V:1]!p! (3(.[bd'].[bd'].[bd']) (3(.[bd'].[bd'].[bd']) (3(.[bd'].[bd'].[bd'])|[V:3][K:treble] (3(.[^GBd].[GBd].[GBd]) (3(.[GBd].[GBd].[GBd]) (3(.[GBd].[GBd].[GBd])|[V:4][K:treble] x6|\n",
      "[r:53/82][V:1](3(.[a^c'].[ac'].[ac']) (3(.[gb].[gb].[gb]) (3(.[^fa].[fa].[fa])|[V:3](3(.[A^c].[Ac].[Ac]) (3(.[GB].[GB].[GB]) (3(.[^FA].[FA].[FA])|\n",
      "[r:54/81][V:1](3(.[e^g].[eg].[eg]) (3(.[d^f].[df].[df]) (3(.[^ce].[ce].[ce])|[V:3][K:bass] (3(.[E^G].[EG].[EG]) (3(.[D^F].[DF].[DF]) (3(.[^CE].[CE].[CE])|[V:4][K:bass] x6|\n",
      "[r:55/80][V:1]!f! (3(.[Bd].[Bd].[Bd]) (3(.[A^c].[Ac].[Ac]) (3(.[^GB].[GB].[GB])|[V:3](3(.[B,D].[B,D].[B,D]) (3(.[A,^C].[A,C].[A,C]) (3(.[^G,B,].[G,B,].[G,B,])|\n",
      "[r:56/79][V:1](3(A^F^G (3AB^c (3de^f)|[V:3][^F,A,]2 z2 z2|\n",
      "[r:57/78][V:1](3(e^f^g (3ab^c' (3d'e'^f')|\n",
      "[r:58/77][V:1]e'4 (T^g2{^fg)}|[V:3][K:treble] (3(EA^c) (3(EAc) (3(EBd)|[V:4][K:treble] x6|\n",
      "[r:59/76][V:1]a2 z2 z2|[V:3](3(A^cA (3EAE (3^CEC)|\n",
      "[r:60/75][V:1]!p! (e2 a2 ^g2)|[V:3][K:bass] (3(A,^CE (3A,CE (3B,DE)|[V:4][K:bass] x6|\n",
      "[r:61/74][V:1](a2 e2 d2)|[V:3](3(A,^CE (3A,CE (3B,DE)|\n",
      "[r:62/73][V:1](^c2 d2 B2)|[V:3](3(A,^CE (3A,CE (3A,DE)|\n",
      "[r:63/72][V:1](A2 E2 D2)|[V:3](3(A,^CE (3A,,^C,E, (3B,,D,E,)|\n",
      "[r:64/71][V:1](^C2 D2 B,2)|[V:3](3(A,,^C,E, (3A,,C,E, (3A,,D,E,)|\n",
      "[r:65/70][V:1]!f! A,2 [EA^c]2 [EAc]2|[V:3][A,,^C,E,]2 [A,,A,]2 [A,,A,]2|\n",
      "[r:66/69][V:1][EA^c]4 z2:|[V:3][A,,A,]4 z2:|\n",
      "[r:67/68][V:1][M:3/4]\"^Trio\"!p! (A2|[V:2][M:3/4] x2|[V:3][M:3/4] z2|[V:4][M:3/4] x2|\n",
      "[r:68/67][V:1]c4 d2)|[V:3](A,2 C2 B,2)|\n",
      "[r:69/66][V:1](f2 e2 d2)|[V:3](D2 C2 B,2)|\n",
      "[r:70/65][V:1](c2 d2 B2)|[V:3](A,2 F,2 D,2)|\n",
      "[r:71/64][V:1](A2 ^G2) (E2|[V:3]E,2 E,,2 z2|\n",
      "[r:72/63][V:1]A4 B2)|[V:3]z2 (A,2 ^G,2)|\n",
      "[r:73/62][V:1](c2 ^c2 d2)|[V:3](A,2 G,2 F,2)|\n",
      "[r:74/61][V:1](^degf=dB)|[V:3](E,2 D,2 G,2)|\n",
      "[r:75/60][V:1]c2 z2::[V:3]C,2 C2::\n",
      "[r:76/59][V:1](e2|\n",
      "[r:77/58][V:1]g4 a2)|[V:3][K:treble] (C2 E2 ^F2)|[V:4][K:treble] x6|\n",
      "[r:78/57][V:1](c'2 b2 a2)|[V:3](G2 ^F2 E2)|\n",
      "[r:79/56][V:1](g2 a2 ^f2)|[V:3](D2 C2 A,2)|\n",
      "[r:80/55][V:1](e2 ^d2) (B2|[V:3][K:bass] B,2 B,,2 z2|[V:4][K:bass] x6|\n",
      "[r:81/54][V:1]e4 ^f2)|[V:3][K:treble] z2 (E2 ^D2)|[V:4][K:treble] x6|\n",
      "[r:82/53][V:1](g2 ^g2 a2)|[V:3](E2 D2 C2)|\n",
      "[r:83/52][V:1](^abd'c'=a^f)|[V:3][K:bass] (B,2 A,2 D2)|[V:4][K:bass] x6|\n",
      "[r:84/51][V:1]g2 z2 (G2|[V:3]G,2 G,,2 z2|\n",
      "[r:85/50][V:1]c4 d2)|[V:3](A,2 C2 B,2)|\n",
      "[r:86/49][V:1](f2 e2 d2)|[V:3](D2 C2 B,2)|\n",
      "[r:87/48][V:1](c2 d2 B2)|[V:3](A,2 F,2 D,2)|\n",
      "[r:88/47][V:1](A2 ^G2) (E2|[V:3]E,2 E,,2 z2|\n",
      "[r:89/46][V:1]A4 B2)|[V:3]z2 (A,2 ^G,2)|\n",
      "[r:90/45][V:1](c2 ^c2 d2)|[V:3](A,2 G,2 F,2)|\n",
      "[r:91/44][V:1](^degf=dB)|[V:3](E,2 D,2 G,2)|\n",
      "[r:92/43][V:1]c2 z2:|[V:3]C,2 C2:|\n",
      "[r:93/42][V:1][M:3/4]\"^Trio\"!p! (3(Gce)|[V:2][M:3/4] x2|[V:3][M:3/4] z2|[V:4][M:3/4] x2|\n",
      "[r:94/41][V:1](g4 f/e/d/c/)|[V:3][K:treble] (3CEG (3CEG (3CEG|[V:4][K:treble] x6|\n",
      "[r:95/40][V:1](B2 A2) (3(Gce)|[V:3](3DFG (3CEG z2|\n",
      "[r:96/39][V:1](g4 f/e/d/c/)|[V:3](3CEG (3CEG (3CEG|\n",
      "[r:97/38][V:1](B2 c2) (3(Gce)|[V:3](3DFG (3CEG z2|\n",
      "[r:98/37][V:1]!f! (g4 f/e/d/c/)|[V:3][K:bass] (3E,G,C (3E,G,C (3E,G,C|[V:4][K:bass] x6|\n",
      "[r:99/36][V:1](B2 A2) (3(afd)|[V:3](3F,A,C (3F,A,C (3F,A,D|\n",
      "[r:100/35][V:1](c4 ed)|[V:2]x4 B2|[V:3](3G,CE (3G,CE (3G,DF|\n",
      "[r:101/34][V:1]c2 z2::[V:3][CE]2 C,2::\n",
      "[r:102/33][V:1]!p! (3(GBd)|\n",
      "[r:103/32][V:1]g2- (3(gag (3fed)|[V:3](3B,DG (3B,DG (3B,DG|\n",
      "[r:104/31][V:1](^d2 e2) (3(Gce)|[V:3](3CEG (3CEG z2|\n",
      "[r:105/30][V:1]g2- (3(gag (3fed)|[V:3](3B,DG (3B,DG (3B,DG|\n",
      "[r:106/29][V:1](^d2 e2)!f! (3(Gce)|[V:3](3CEG (3CEG z2|\n",
      "[r:107/28][V:1](g4 f/e/d/c/)|[V:3](3E,G,C (3E,G,C (3E,G,C|\n",
      "[r:108/27][V:1](B2 A2) (3(afd)|[V:3](3F,A,C (3F,A,C (3F,A,D|\n",
      "[r:109/26][V:1](c4 ed)|[V:2]x4 B2|[V:3](3G,CE (3G,CE (3G,DF|\n",
      "[r:110/25][V:1]c2 z2 z2:|[V:3][CE]2 C,2 z2:|\n",
      "[r:111/24][V:1][M:3/4]\"^Trio\"!p! G2 c2 e2|[V:2][M:3/4] x6|[V:3][M:3/4] (3CEG (3CEG (3CEG|[V:4][M:3/4] x6|\n",
      "[r:112/23][V:1](g4 f2)|[V:3](3B,DG (3B,DG (3B,DG|\n",
      "[r:113/22][V:1]e2 (3(gfe (3dcB)|[V:3](3CEG (3CEG (3CEG|\n",
      "[r:114/21][V:1](c4 B2)|[V:3](3DFG (3DFG (3DFG|\n",
      "[r:115/20][V:1](c2 e2 g2)|[V:3](3EGE (3CEC (3E,G,C|\n",
      "[r:116/19][V:1](g2 B2) c2|[V:3](3F,G,D (3F,G,D (3E,G,C|\n",
      "[r:117/18][V:1](3(^cdf) =c2 TB2|[V:3](3F,A,D (3G,CE (3G,DF|\n",
      "[r:118/17][V:1]c2 z2 z2::[V:3](3CEG (3CEG C2::\n",
      "[r:119/16][V:1]!p! (3(GBc) (3(dcB) (3(fed)|[V:3][K:treble] (3(FGF (3GFG (3FGF)|[V:4][K:treble] x6|\n",
      "[r:120/15][V:1](3(gef) (3(gab) (3(c'ge)|[V:3](3(EGE (3GEG (3EGE)|\n",
      "[r:121/14][V:1](3(GBc) (3(dcB) (3(fed)|[V:3](3(FGF (3GFG (3FGF)|\n",
      "[r:122/13][V:1](3(gef) (3(gab) (3(c'ge)|[V:3](3(EGE (3GEG (3EGE)|\n",
      "[r:123/12][V:1]!f! (3(^de^f (3gab (3c'ba)|[V:3][B,^FA]6|\n",
      "[r:124/11][V:1](3(gba (3g^fe (3dcB)|[V:3][EG]2 z2 z2|\n",
      "[r:125/10][V:1](3(Ace) G2 T^F2|[V:3][K:bass] z2 B,2 C2|[V:4][K:bass] x2 D,4|\n",
      "[r:126/9][V:1](3(GAG) (3(FED) (3(CB,A,)|[V:3][G,B,]2 z2 z2|\n",
      "[r:127/8][V:1]!p! G2 c2 e2|[V:3](3CEG (3CEG (3CEG|\n",
      "[r:128/7][V:1](g4 f2)|[V:3](3B,DG (3B,DG (3B,DG|\n",
      "[r:129/6][V:1]e2 (3(gfe (3dcB)|[V:3](3CEG (3CEG (3CEG|\n",
      "[r:130/5][V:1](c4 B2)|[V:3](3DFG (3DFG (3DFG|\n",
      "[r:131/4][V:1](c2 e2 g2)|[V:3](3EGE (3CEC (3E,G,C|\n",
      "[r:132/3][V:1](g2 B2) c2|[V:3](3F,G,D (3F,G,D (3E,G,C|\n",
      "[r:133/2][V:1](3(^cdf) =c2 TB2|1[V:3](3F,A,D (3G,CE (3G,DF|1\n",
      "[r:134/1][V:1]c2 z2 z2:|2[V:3](3CEG (3CEG C2:|2\n",
      "[r:135/0][V:1]c2 z2|][V:3][CE]2 z2|]\n",
      "-- decoded from utf-8\n",
      "-- skipped header: (field X,1)\n",
      "./Classical_Beethoven_Ludwig_van_Keyboard_0.xml written\n",
      "-- done in 0.64 secs\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    period, composer, instrumentation = 'Classical', 'Beethoven, Ludwig van', 'Keyboard'\n",
    "    default_note, tempo, meter, key = \"1/8\", \"1/4=60\", \"3/4\", \"C\"\n",
    "    # score = \"1 { ( 2 4 ) | ( 3 5 ) }\"\n",
    "    score = \"{ ( 1 2 ) | ( 3 4 ) }\"\n",
    "    result = inference_patch(period, composer, instrumentation, [f\"%%score {score} \\n\", f\"L:{default_note}\\n\", f\"Q:{tempo}\\n\", f\"M:{meter}\\n\", f\"K:{key}\\n\"])\n",
    "\n",
    "    abc_lines = result.splitlines()\n",
    "    abc_lines = [line + '\\n' for line in abc_lines if line.strip()]  # Add newlines and remove empty lines\n",
    "\n",
    "    abc_lines = rest_unreduce(abc_lines)\n",
    "\n",
    "    # Construct unique file name\n",
    "    filename = f\"{sanitize(period)}_{sanitize(composer)}_{sanitize(instrumentation)}_{i}.abc\"\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(abc_lines)\n",
    "    !python abc2xml.py -o . {filename}\n",
    "except Exception as e:\n",
    "    print(f\"Exception caught {e}, skipping {sanitize(period)}_{sanitize(composer)}_{sanitize(instrumentation)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
